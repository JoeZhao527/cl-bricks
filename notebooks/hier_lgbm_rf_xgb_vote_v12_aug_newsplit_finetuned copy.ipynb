{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "import warnings\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tsfel\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from hiclass import LocalClassifierPerNode, LocalClassifierPerParentNode, LocalClassifierPerLevel\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output model name\n",
    "model_name = 'model_10012025_featsel_aug.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output submission name\n",
    "submission_name = \"hier_lgbrfvote_improve_tier_10_fold_neg_sample_thr_0110_featsel_aug_leo.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../downloads/train_data_features_v3_fixed\"\n",
    "train_y_path = \"../downloads/train_y_v0.1.0.csv\"\n",
    "test_feat_path = \"../downloads/test_features_full_v3.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(train_y, n_splits=10):\n",
    "    # Create a label array for stratification\n",
    "    # We'll use the first non-zero label for each row as the stratification target\n",
    "    stratify_labels = []\n",
    "    for _, row in tqdm(train_y.iterrows(), total=len(train_y)):\n",
    "        labels = row[train_y.columns != 'filename'].values\n",
    "        stratify_labels.append(str(labels))\n",
    "    \n",
    "    # Create StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Generate fold indices\n",
    "    folds = []\n",
    "    for train_idx, val_idx in skf.split(train_y, stratify_labels):\n",
    "        folds.append({\n",
    "            'train': train_idx,\n",
    "            'val': val_idx\n",
    "        })\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['0_Absolute energy',\n",
    " '0_Area under the curve',\n",
    " '0_Autocorrelation',\n",
    " '0_Average power',\n",
    " '0_Centroid',\n",
    " '0_ECDF Percentile Count_0',\n",
    " '0_ECDF Percentile Count_1',\n",
    " '0_ECDF Percentile_0',\n",
    " '0_ECDF Percentile_1',\n",
    " '0_ECDF_0',\n",
    " '0_ECDF_1',\n",
    " '0_ECDF_2',\n",
    " '0_ECDF_3',\n",
    " '0_ECDF_4',\n",
    " '0_ECDF_5',\n",
    " '0_ECDF_6',\n",
    " '0_ECDF_7',\n",
    " '0_ECDF_8',\n",
    " '0_ECDF_9',\n",
    " '0_Entropy',\n",
    " '0_Histogram mode',\n",
    " '0_Interquartile range',\n",
    " '0_Kurtosis',\n",
    " '0_Max',\n",
    " '0_Mean',\n",
    " '0_Mean absolute deviation',\n",
    " '0_Mean absolute diff',\n",
    " '0_Mean diff',\n",
    " '0_Median',\n",
    " '0_Median absolute deviation',\n",
    " '0_Median absolute diff',\n",
    " '0_Median diff',\n",
    " '0_Min',\n",
    " '0_Negative turning points',\n",
    " '0_Neighbourhood peaks',\n",
    " '0_Peak to peak distance',\n",
    " '0_Positive turning points',\n",
    " '0_Root mean square',\n",
    " '0_Signal distance',\n",
    " '0_Skewness',\n",
    " '0_Slope',\n",
    " '0_Standard deviation',\n",
    " '0_Sum absolute diff',\n",
    " '0_Variance',\n",
    " '0_Zero crossing rate',\n",
    " '0_Fundamental frequency',\n",
    " '0_Human range energy',\n",
    " '0_Max power spectrum',\n",
    " '0_Maximum frequency',\n",
    " '0_Median frequency',\n",
    " '0_Power bandwidth',\n",
    " '0_Wavelet entropy',\n",
    " 'value_median',\n",
    " 'value_mean',\n",
    " 'value_qmean',\n",
    " 'value_max',\n",
    " 'value_min',\n",
    " 'value_maxmin',\n",
    " 'value_diffmax',\n",
    " 'value_diffmin',\n",
    " 'value_diffmean',\n",
    " 'value_diffqmean',\n",
    " 'value_diffmedian',\n",
    " 'value_diffmaxmin',\n",
    " 'time_diffmean',\n",
    " 'time_diffqmean',\n",
    " 'time_diffmax',\n",
    " 'time_diffmin',\n",
    " 'time_diffmedian',\n",
    " 'value_std',\n",
    " 'value_var',\n",
    " 'value_diffstd',\n",
    " 'value_diffvar',\n",
    " 'time_diffstd',\n",
    " 'time_diffvar',\n",
    " 'time_burstiness',\n",
    " 'time_total',\n",
    " 'time_event_density',\n",
    " 'time_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../downloads/train_data_features_v3_fixed/train_features_full_v3.csv',\n",
       " '../downloads/train_data_features_v3_fixed/train_features_split1_2_v3.csv',\n",
       " '../downloads/train_data_features_v3_fixed/train_features_split1_3_v3.csv',\n",
       " '../downloads/train_data_features_v3_fixed/train_features_split1_4_v3.csv',\n",
       " '../downloads/train_data_features_v3_fixed/train_features_split2_2_v3.csv',\n",
       " '../downloads/train_data_features_v3_fixed/train_features_split2_3_v3.csv']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = [os.path.join(train_dir, f) for f in sorted(os.listdir(train_dir))]\n",
    "train_files = train_files[:6]\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets = [pd.read_csv(f) for f in train_files]\n",
    "\n",
    "feature_list = train_sets[0].columns.tolist()\n",
    "feature_list = [item for item in feature_list if \"LPCC\" not in item]\n",
    "feature_list = [item for item in feature_list if \"MFCC\" not in item]\n",
    "\n",
    "train_sets = [_df[feature_list] for _df in train_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(train_y_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31839/31839 [00:15<00:00, 2094.77it/s]\n"
     ]
    }
   ],
   "source": [
    "folds = create_folds(train_y, n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.read_csv(test_feat_path)[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.concat(train_sets, ignore_index=True).drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_labels_np(row):\n",
    "    \"\"\"More efficient version using numpy\"\"\"\n",
    "    arr = row.to_numpy() # convert to numpy array\n",
    "    indices = np.where(arr == 1)[0] # get indices where value is 1\n",
    "    labels = row.index[indices].tolist() # get labels from indices\n",
    "    return labels\n",
    "\n",
    "labelhir = train_y.apply(get_active_labels_np, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a tier dict\n",
    "ontology_list = list(train_y.columns[1:])\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_tree(onto):\n",
    "    \"\"\"\n",
    "    Build a tree so that each term has at most one parent.\n",
    "    The parent is determined by the longest existing term that is a substring of the child.\n",
    "    \"\"\"\n",
    "    # Sort terms by length so that broader terms are processed (and assigned) first\n",
    "    sorted_onto = sorted(onto, key=len)\n",
    "    \n",
    "    # Dictionaries for storing parent-child relationships\n",
    "    parent_map = {}             # term -> parent\n",
    "    children_map = defaultdict(list)  # parent -> [children]\n",
    "\n",
    "    processed = []\n",
    "    \n",
    "    for term in sorted_onto:\n",
    "        # Find all processed terms that are substrings of 'term'\n",
    "        potential_parents = [p for p in processed if p in term]\n",
    "        \n",
    "        if not potential_parents:\n",
    "            # No parent found; this term is at the root\n",
    "            parent_map[term] = None\n",
    "            children_map[None].append(term)\n",
    "        else:\n",
    "            # Pick the longest parent (closest match)\n",
    "            parent = max(potential_parents, key=len)\n",
    "            parent_map[term] = parent\n",
    "            children_map[parent].append(term)\n",
    "        \n",
    "        processed.append(term)\n",
    "    \n",
    "    return parent_map, children_map\n",
    "\n",
    "\"\"\"\n",
    "Re-built hierachical labels\n",
    "\"\"\"\n",
    "level_labels = [[], [], [], [], []]\n",
    "\n",
    "def print_tree(children_map, root=None, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively print the tree structure with indentation.\n",
    "    'root=None' means we are listing top-level (root) terms first.\n",
    "    \"\"\"\n",
    "    \n",
    "    if root is None:\n",
    "        # For all top-level terms\n",
    "        for child in sorted(children_map[root]):\n",
    "            print_tree(children_map, child, depth)\n",
    "    else:\n",
    "        # print(\"  \" * depth + root)\n",
    "        level_labels[depth].append(root)\n",
    "        for child in sorted(children_map[root]):\n",
    "            print_tree(children_map, child, depth + 1)\n",
    "\n",
    "parent_map, children_map = build_tree(ontology_list)\n",
    "print_tree(children_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiers = {\n",
    "    1: level_labels[0],\n",
    "    2: level_labels[1],\n",
    "    3: level_labels[2],\n",
    "    4: level_labels[3],\n",
    "    5: level_labels[4]\n",
    "}\n",
    "\n",
    "def get_tier(label):\n",
    "    for tier_num, tier_list in tiers.items():\n",
    "        if label in tier_list:\n",
    "            return tier_num\n",
    "    return None  # Handle cases where the label isn't found in any tier\n",
    "\n",
    "def sort_labels(labels):\n",
    "    return sorted(labels, key=lambda label: (get_tier(label) or float('inf'), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_labelhir = [sort_labels(labels) for labels in labelhir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_hier = np.array(\n",
    "    sorted_labelhir,\n",
    "    dtype=object,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31839"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "_label = np.array([x[-1] for x in label_hier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31839"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_label = pd.Series(label_hier).apply(lambda x: x + ['None'] * (5 - len(x)) if len(x) < 5 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31839"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1: 0 None values out of 31839 total (0.00%)\n",
      "Level 2: 12321 None values out of 31839 total (38.70%)\n",
      "Level 3: 20247 None values out of 31839 total (63.59%)\n",
      "Level 4: 27216 None values out of 31839 total (85.48%)\n",
      "Level 5: 30936 None values out of 31839 total (97.16%)\n"
     ]
    }
   ],
   "source": [
    "# Count Nones at each level\n",
    "for i in range(5):\n",
    "    none_count = sum(padded_label.apply(lambda x: x[i] == 'None'))\n",
    "    print(f\"Level {i+1}: {none_count} None values out of {len(padded_label)} total ({none_count/len(padded_label):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm_classifier(train_X, _label, folds, drop_none=False):\n",
    "    \"\"\"\n",
    "    Train LightGBM models using k-fold cross validation\n",
    "\n",
    "    Args:\n",
    "        train_X: Training features DataFrame\n",
    "        _label: Array of labels\n",
    "        folds: List of dictionaries containing train/val indices\n",
    "        drop_none: Whether to drop samples with \"None\" labels\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of trained classifiers, list of scores, list of validation predictions)\n",
    "    \"\"\"\n",
    "    classifiers = []\n",
    "    scores = []\n",
    "    val_predictions = []  # List to store validation predictions\n",
    "\n",
    "    # Define LightGBM parameters\n",
    "    params = {\n",
    "        'verbose':-1,\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': 8,  # Use all available cores\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': len(set(_label))  # Set number of classes based on unique labels\n",
    "    }\n",
    "\n",
    "    for f_idx, fold in enumerate(folds):\n",
    "        # Prepare train and validation data for this fold\n",
    "        train_X_fold = train_X.iloc[fold['train']]\n",
    "        train_y_fold = _label[fold['train']]\n",
    "        val_X_fold = train_X.iloc[fold['val']]\n",
    "        val_y_fold = _label[fold['val']]\n",
    "\n",
    "        if drop_none:\n",
    "            # Remove samples with \"None\" labels from training set\n",
    "            train_mask = train_y_fold != \"None\"\n",
    "            train_X_fold = train_X_fold[train_mask]\n",
    "            train_y_fold = train_y_fold[train_mask]\n",
    "\n",
    "            # Remove samples with \"None\" labels from validation set\n",
    "            val_mask = val_y_fold != \"None\"\n",
    "            val_X_fold = val_X_fold[val_mask]\n",
    "            val_y_fold = val_y_fold[val_mask]\n",
    "            print(f\"Dropped train: {len(train_X_fold) - sum(train_mask)}, val: {len(val_X_fold) - sum(val_mask)}\")\n",
    "\n",
    "        # Check the train_y_fold. If more than 30% of samples are labeled \"None\",\n",
    "        # randomly sample from the \"None\" to make that ratio no more than 30%.\n",
    "        none_mask = (train_y_fold == \"None\")\n",
    "        none_count = np.sum(none_mask)\n",
    "        total_samples = len(train_y_fold)\n",
    "        none_ratio = none_count / total_samples if total_samples > 0 else 0\n",
    "\n",
    "        if none_ratio > 0.4:\n",
    "            # Calculate how many \"None\" labels we should keep (30% of total)\n",
    "            max_none_to_keep = int(0.4 * (total_samples - none_count))\n",
    "\n",
    "            # Randomly choose which \"None\" labels to keep\n",
    "            none_indices = np.where(none_mask)[0]\n",
    "\n",
    "            # Fix the random seed before shuffling for reproducibility\n",
    "            rng = np.random.RandomState(f_idx)\n",
    "            rng.shuffle(none_indices)\n",
    "\n",
    "            keep_none_indices = none_indices[:max_none_to_keep]\n",
    "\n",
    "            # Indices of all non-\"None\" labels\n",
    "            other_indices = np.where(~none_mask)[0]\n",
    "\n",
    "            # Combine indices to keep and then sort\n",
    "            new_indices = np.concatenate([keep_none_indices, other_indices])\n",
    "            new_indices = np.sort(new_indices)  # Sort so we can index the DataFrame consistently\n",
    "\n",
    "            # Subset the training data\n",
    "            train_X_fold = train_X_fold.iloc[new_indices]\n",
    "            train_y_fold = train_y_fold[new_indices]\n",
    "\n",
    "            print(f\"Sampled: none-ratio: {none_ratio}, removed: {total_samples - max_none_to_keep}\")\n",
    "\n",
    "        # Create and train LightGBM model\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(train_X_fold, train_y_fold)\n",
    "\n",
    "        classifiers.append(model)\n",
    "\n",
    "        # Calculate score and save predictions on validation set\n",
    "        val_preds = model.predict(val_X_fold)\n",
    "        score = np.mean(val_preds == val_y_fold)\n",
    "        scores.append(score)\n",
    "        val_predictions.append({\n",
    "            'true_labels': val_y_fold,\n",
    "            'predicted_labels': val_preds,\n",
    "            'fold_indices': fold['val']\n",
    "        })\n",
    "        print(f\"Fold score: {score:.4f}\")\n",
    "\n",
    "    print(f\"Average score: {np.mean(scores)}\")\n",
    "    return classifiers, scores, val_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the high precision model by allowing None prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training level 0\n",
      "Fold score: 0.8278\n",
      "Fold score: 0.8268\n",
      "Fold score: 0.8282\n",
      "Fold score: 0.8298\n",
      "Fold score: 0.8291\n",
      "Average score: 0.8283457395018686\n",
      "Training level 1\n",
      "Fold score: 0.2406\n",
      "Fold score: 0.4173\n",
      "Fold score: 0.2397\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     _classifiers, _scores, _val_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lgbm_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpadded_label\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     prec_lgbm_classifiers\u001b[38;5;241m.\u001b[39mappend(_classifiers)\n\u001b[1;32m      9\u001b[0m     prec_scores\u001b[38;5;241m.\u001b[39mappend(_scores)\n",
      "Cell \u001b[0;32mIn[67], line 83\u001b[0m, in \u001b[0;36mtrain_lgbm_classifier\u001b[0;34m(train_X, _label, folds, drop_none)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Create and train LightGBM model\u001b[39;00m\n\u001b[1;32m     82\u001b[0m model \u001b[38;5;241m=\u001b[39m LGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m classifiers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Calculate score and save predictions on validation set\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddm/lib/python3.9/site-packages/lightgbm/sklearn.py:1284\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1282\u001b[0m             valid_sets\u001b[38;5;241m.\u001b[39mappend((valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y)))\n\u001b[0;32m-> 1284\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddm/lib/python3.9/site-packages/lightgbm/sklearn.py:955\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    952\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    953\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mbest_iteration\n",
      "File \u001b[0;32m~/miniconda3/envs/ddm/lib/python3.9/site-packages/lightgbm/engine.py:307\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    296\u001b[0m     cb(\n\u001b[1;32m    297\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    298\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 307\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddm/lib/python3.9/site-packages/lightgbm/basic.py:4136\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4135\u001b[0m _safe_call(\n\u001b[0;32m-> 4136\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4140\u001b[0m )\n\u001b[1;32m   4141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prec_lgbm_classifiers = []\n",
    "prec_scores = []\n",
    "prec_lgbm_val_predictions = []\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Training level {i}\")\n",
    "    _classifiers, _scores, _val_predictions = train_lgbm_classifier(train_X, np.array([x[i] for x in padded_label]), folds, drop_none=False)\n",
    "    prec_lgbm_classifiers.append(_classifiers)\n",
    "    prec_scores.append(_scores)\n",
    "    prec_lgbm_val_predictions.append(_val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = np.finfo(np.float32).max\n",
    "cliped_text_X = np.clip(test_X, -max_value, max_value)\n",
    "\n",
    "cliped_text_X = cliped_text_X.replace([np.inf, -np.inf], 0.0)  # Replace infinity\n",
    "cliped_text_X = cliped_text_X.fillna(0.0)  # Replace NaN\n",
    "\n",
    "print(\"Checking for NaN values:\", np.any(np.isnan(cliped_text_X)))\n",
    "print(\"Checking for infinite values:\", np.any(np.isinf(cliped_text_X)))\n",
    "print(\"Maximum value:\", np.max(cliped_text_X))\n",
    "print(\"Minimum value:\", np.min(cliped_text_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_with_models(classifiers, test_data):\n",
    "    \"\"\"\n",
    "    Make probability predictions using multiple classifier models\n",
    "    \n",
    "    Args:\n",
    "        classifiers: List of trained classifier models\n",
    "        test_data: Test data to make predictions on\n",
    "        \n",
    "    Returns:\n",
    "        List of probability predictions from each classifier\n",
    "    \"\"\"\n",
    "    test_preds_all = []\n",
    "    for clf in tqdm(classifiers):\n",
    "        pred = clf.predict_proba(test_data)\n",
    "        test_preds_all.append(pred)\n",
    "    return test_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_combine_predictions(classifiers, test_preds_all, test_data, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Aligns predictions from multiple classifiers and combines them through averaging\n",
    "    \n",
    "    Args:\n",
    "        classifiers: List of trained classifier models\n",
    "        test_preds_all: List of probability predictions from each classifier\n",
    "        test_data: Test data used for predictions\n",
    "        threshold: Minimum probability threshold for making predictions\n",
    "        \n",
    "    Returns:\n",
    "        Final class predictions after aligning and combining probabilities\n",
    "    \"\"\"\n",
    "    # Get the common classes across all classifiers\n",
    "    all_classes = classifiers[0].classes_\n",
    "    test_preds_aligned = []\n",
    "\n",
    "    # Make predictions with each fold's model and align them \n",
    "    for i, clf in tqdm(enumerate(classifiers)):\n",
    "        pred = test_preds_all[i]\n",
    "        # Create a mapping to align predictions with common classes\n",
    "        pred_dict = {_cls: idx for idx, _cls in enumerate(clf.classes_)}\n",
    "        aligned_pred = np.zeros((len(test_data), len(all_classes)))\n",
    "        \n",
    "        for i, _cls in enumerate(all_classes):\n",
    "            if _cls in pred_dict:\n",
    "                aligned_pred[:, i] = pred[:, pred_dict[_cls]]\n",
    "        \n",
    "        test_preds_aligned.append(aligned_pred)\n",
    "\n",
    "    # Stack and average the aligned predictions\n",
    "    test_preds_all = np.stack(test_preds_aligned)\n",
    "    test_preds_proba = test_preds_all.mean(axis=0)\n",
    "\n",
    "    # Get max probabilities for each prediction\n",
    "    max_probs = np.max(test_preds_proba, axis=1)\n",
    "    \n",
    "    # Convert probabilities to class predictions, using threshold\n",
    "    test_preds = np.array(['None'] * len(test_data), dtype=object)\n",
    "    confident_mask = max_probs >= threshold\n",
    "    test_preds[confident_mask] = all_classes[np.argmax(test_preds_proba[confident_mask], axis=1)]\n",
    "    \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "thr_list = [0.0, 0.2, 0.3, 0.4, 0.5]\n",
    "for i in range(5):\n",
    "    print(f\"Predicting level {i}\")\n",
    "    test_preds_all = make_predictions_with_models(prec_classifiers[i], cliped_text_X)\n",
    "    test_preds.append(align_and_combine_predictions(prec_classifiers[i], test_preds_all, cliped_text_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Loop through levels\n",
    "# for level in range(len(prec_lgbm_classifiers)):  # Iterate over levels (5 in this case)\n",
    "#     classifiers = prec_lgbm_classifiers[level]  # List of classifiers for this level\n",
    "#     for fold_idx, classifier in enumerate(classifiers):  # Iterate over folds\n",
    "#         # Define the file path for this classifier\n",
    "#         file_path = f\"./lgbm_classifier_level_4_fold_{fold_idx}.pkl\"\n",
    "        \n",
    "#         # Save the classifier using pickle\n",
    "#         with open(file_path, \"wb\") as f:\n",
    "#             pickle.dump(classifier, f)\n",
    "#         print(f\"Saved LGBM classifier for level 4, fold {fold_idx} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training level 0\n",
      "Fold score: 0.8322\n",
      "Fold score: 0.8350\n",
      "Fold score: 0.8347\n",
      "Fold score: 0.8357\n",
      "Fold score: 0.8348\n",
      "Fold score: 0.8358\n",
      "Fold score: 0.8349\n",
      "Fold score: 0.8354\n",
      "Fold score: 0.8347\n",
      "Fold score: 0.8350\n",
      "Average score: 0.834837777568391\n",
      "Training level 1\n",
      "Fold score: 0.9135\n",
      "Fold score: 0.9145\n",
      "Fold score: 0.9160\n",
      "Fold score: 0.9141\n",
      "Fold score: 0.9167\n",
      "Fold score: 0.9170\n",
      "Fold score: 0.9147\n",
      "Fold score: 0.9171\n",
      "Fold score: 0.9142\n",
      "Fold score: 0.9161\n",
      "Average score: 0.9153679449731461\n",
      "Training level 2\n",
      "Sampled: none-ratio: 0.6358588872486922, removed: 244813\n",
      "Fold score: 0.8957\n",
      "Sampled: none-ratio: 0.6359007646108371, removed: 244818\n",
      "Fold score: 0.8963\n",
      "Sampled: none-ratio: 0.6359740499945908, removed: 244827\n",
      "Fold score: 0.8984\n",
      "Sampled: none-ratio: 0.6358937850504797, removed: 244817\n",
      "Fold score: 0.8970\n",
      "Sampled: none-ratio: 0.6358658668090497, removed: 244814\n",
      "Fold score: 0.8998\n",
      "Sampled: none-ratio: 0.6360298864774507, removed: 244833\n",
      "Fold score: 0.8982\n",
      "Sampled: none-ratio: 0.636012437576557, removed: 244831\n",
      "Fold score: 0.8998\n",
      "Sampled: none-ratio: 0.635918213511731, removed: 244820\n",
      "Fold score: 0.8968\n",
      "Sampled: none-ratio: 0.6358798259297647, removed: 244816\n",
      "Fold score: 0.8944\n",
      "Sampled: none-ratio: 0.6358484179081559, removed: 244812\n",
      "Fold score: 0.8972\n",
      "Average score: 0.8973711485913503\n",
      "Training level 3\n",
      "Sampled: none-ratio: 0.8547553489605689, removed: 269903\n",
      "Fold score: 0.9266\n",
      "Sampled: none-ratio: 0.8545913292921679, removed: 269885\n",
      "Fold score: 0.9292\n",
      "Sampled: none-ratio: 0.8549088992884338, removed: 269921\n",
      "Fold score: 0.9279\n",
      "Sampled: none-ratio: 0.8548530628055738, removed: 269915\n",
      "Fold score: 0.9276\n",
      "Sampled: none-ratio: 0.8547727978614627, removed: 269905\n",
      "Fold score: 0.9269\n",
      "Sampled: none-ratio: 0.8548600423659314, removed: 269915\n",
      "Fold score: 0.9278\n",
      "Sampled: none-ratio: 0.854933327749685, removed: 269924\n",
      "Fold score: 0.9296\n",
      "Sampled: none-ratio: 0.8547379000596752, removed: 269901\n",
      "Fold score: 0.9285\n",
      "Sampled: none-ratio: 0.8547762876416415, removed: 269906\n",
      "Fold score: 0.9285\n",
      "Sampled: none-ratio: 0.8548181650037864, removed: 269911\n",
      "Fold score: 0.9321\n",
      "Average score: 0.928461949181821\n",
      "Training level 4\n",
      "Sampled: none-ratio: 0.9716350667071482, removed: 283300\n",
      "Fold score: 0.9571\n",
      "Sampled: none-ratio: 0.9716071484657182, removed: 283297\n",
      "Fold score: 0.9575\n",
      "Sampled: none-ratio: 0.9715827200044669, removed: 283294\n",
      "Fold score: 0.9608\n",
      "Sampled: none-ratio: 0.9716001689053606, removed: 283296\n",
      "Fold score: 0.9585\n",
      "Sampled: none-ratio: 0.9716769440692931, removed: 283305\n",
      "Fold score: 0.9553\n",
      "Sampled: none-ratio: 0.9716385564873269, removed: 283301\n",
      "Fold score: 0.9592\n",
      "Sampled: none-ratio: 0.9716001689053606, removed: 283296\n",
      "Fold score: 0.9550\n",
      "Sampled: none-ratio: 0.9717223112116168, removed: 283310\n",
      "Fold score: 0.9597\n",
      "Sampled: none-ratio: 0.9716839236296506, removed: 283306\n",
      "Fold score: 0.9560\n",
      "Sampled: none-ratio: 0.9716385564873269, removed: 283301\n",
      "Fold score: 0.9598\n",
      "Average score: 0.957884983824869\n"
     ]
    }
   ],
   "source": [
    "# prec_xgb_classifiers = []\n",
    "# prec_scores = []\n",
    "# prec_xgb_val_predictions = []\n",
    "\n",
    "# for i in range(5):\n",
    "#     print(f\"Training level {i}\")\n",
    "#     _classifiers, _scores, _val_predictions = train_xgboost(train_X, np.array([x[i] for x in padded_label]), folds, drop_none=False)\n",
    "#     prec_xgb_classifiers.append(_classifiers)\n",
    "#     prec_scores.append(_scores)\n",
    "#     prec_xgb_val_predictions.append(_val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGBoost classifier for level 0, fold 0 to ./xgb_classifier_level_0_fold_0.pkl\n",
      "Saved XGBoost classifier for level 0, fold 1 to ./xgb_classifier_level_0_fold_1.pkl\n",
      "Saved XGBoost classifier for level 0, fold 2 to ./xgb_classifier_level_0_fold_2.pkl\n",
      "Saved XGBoost classifier for level 0, fold 3 to ./xgb_classifier_level_0_fold_3.pkl\n",
      "Saved XGBoost classifier for level 0, fold 4 to ./xgb_classifier_level_0_fold_4.pkl\n",
      "Saved XGBoost classifier for level 0, fold 5 to ./xgb_classifier_level_0_fold_5.pkl\n",
      "Saved XGBoost classifier for level 0, fold 6 to ./xgb_classifier_level_0_fold_6.pkl\n",
      "Saved XGBoost classifier for level 0, fold 7 to ./xgb_classifier_level_0_fold_7.pkl\n",
      "Saved XGBoost classifier for level 0, fold 8 to ./xgb_classifier_level_0_fold_8.pkl\n",
      "Saved XGBoost classifier for level 0, fold 9 to ./xgb_classifier_level_0_fold_9.pkl\n",
      "Saved XGBoost classifier for level 1, fold 0 to ./xgb_classifier_level_1_fold_0.pkl\n",
      "Saved XGBoost classifier for level 1, fold 1 to ./xgb_classifier_level_1_fold_1.pkl\n",
      "Saved XGBoost classifier for level 1, fold 2 to ./xgb_classifier_level_1_fold_2.pkl\n",
      "Saved XGBoost classifier for level 1, fold 3 to ./xgb_classifier_level_1_fold_3.pkl\n",
      "Saved XGBoost classifier for level 1, fold 4 to ./xgb_classifier_level_1_fold_4.pkl\n",
      "Saved XGBoost classifier for level 1, fold 5 to ./xgb_classifier_level_1_fold_5.pkl\n",
      "Saved XGBoost classifier for level 1, fold 6 to ./xgb_classifier_level_1_fold_6.pkl\n",
      "Saved XGBoost classifier for level 1, fold 7 to ./xgb_classifier_level_1_fold_7.pkl\n",
      "Saved XGBoost classifier for level 1, fold 8 to ./xgb_classifier_level_1_fold_8.pkl\n",
      "Saved XGBoost classifier for level 1, fold 9 to ./xgb_classifier_level_1_fold_9.pkl\n",
      "Saved XGBoost classifier for level 2, fold 0 to ./xgb_classifier_level_2_fold_0.pkl\n",
      "Saved XGBoost classifier for level 2, fold 1 to ./xgb_classifier_level_2_fold_1.pkl\n",
      "Saved XGBoost classifier for level 2, fold 2 to ./xgb_classifier_level_2_fold_2.pkl\n",
      "Saved XGBoost classifier for level 2, fold 3 to ./xgb_classifier_level_2_fold_3.pkl\n",
      "Saved XGBoost classifier for level 2, fold 4 to ./xgb_classifier_level_2_fold_4.pkl\n",
      "Saved XGBoost classifier for level 2, fold 5 to ./xgb_classifier_level_2_fold_5.pkl\n",
      "Saved XGBoost classifier for level 2, fold 6 to ./xgb_classifier_level_2_fold_6.pkl\n",
      "Saved XGBoost classifier for level 2, fold 7 to ./xgb_classifier_level_2_fold_7.pkl\n",
      "Saved XGBoost classifier for level 2, fold 8 to ./xgb_classifier_level_2_fold_8.pkl\n",
      "Saved XGBoost classifier for level 2, fold 9 to ./xgb_classifier_level_2_fold_9.pkl\n",
      "Saved XGBoost classifier for level 3, fold 0 to ./xgb_classifier_level_3_fold_0.pkl\n",
      "Saved XGBoost classifier for level 3, fold 1 to ./xgb_classifier_level_3_fold_1.pkl\n",
      "Saved XGBoost classifier for level 3, fold 2 to ./xgb_classifier_level_3_fold_2.pkl\n",
      "Saved XGBoost classifier for level 3, fold 3 to ./xgb_classifier_level_3_fold_3.pkl\n",
      "Saved XGBoost classifier for level 3, fold 4 to ./xgb_classifier_level_3_fold_4.pkl\n",
      "Saved XGBoost classifier for level 3, fold 5 to ./xgb_classifier_level_3_fold_5.pkl\n",
      "Saved XGBoost classifier for level 3, fold 6 to ./xgb_classifier_level_3_fold_6.pkl\n",
      "Saved XGBoost classifier for level 3, fold 7 to ./xgb_classifier_level_3_fold_7.pkl\n",
      "Saved XGBoost classifier for level 3, fold 8 to ./xgb_classifier_level_3_fold_8.pkl\n",
      "Saved XGBoost classifier for level 3, fold 9 to ./xgb_classifier_level_3_fold_9.pkl\n",
      "Saved XGBoost classifier for level 4, fold 0 to ./xgb_classifier_level_4_fold_0.pkl\n",
      "Saved XGBoost classifier for level 4, fold 1 to ./xgb_classifier_level_4_fold_1.pkl\n",
      "Saved XGBoost classifier for level 4, fold 2 to ./xgb_classifier_level_4_fold_2.pkl\n",
      "Saved XGBoost classifier for level 4, fold 3 to ./xgb_classifier_level_4_fold_3.pkl\n",
      "Saved XGBoost classifier for level 4, fold 4 to ./xgb_classifier_level_4_fold_4.pkl\n",
      "Saved XGBoost classifier for level 4, fold 5 to ./xgb_classifier_level_4_fold_5.pkl\n",
      "Saved XGBoost classifier for level 4, fold 6 to ./xgb_classifier_level_4_fold_6.pkl\n",
      "Saved XGBoost classifier for level 4, fold 7 to ./xgb_classifier_level_4_fold_7.pkl\n",
      "Saved XGBoost classifier for level 4, fold 8 to ./xgb_classifier_level_4_fold_8.pkl\n",
      "Saved XGBoost classifier for level 4, fold 9 to ./xgb_classifier_level_4_fold_9.pkl\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Loop through levels\n",
    "# for level in range(len(prec_xgb_classifiers)):  # Iterate over levels (5 in this case)\n",
    "#     classifiers = prec_xgb_classifiers[level]  # List of classifiers for this level\n",
    "#     for fold_idx, classifier in enumerate(classifiers):  # Iterate over folds\n",
    "#         # Define the file path for this classifier\n",
    "#         file_path = f\"./xgb_classifier_level_{level}_fold_{fold_idx}.pkl\"\n",
    "        \n",
    "#         # Save the classifier using pickle\n",
    "#         with open(file_path, \"wb\") as f:\n",
    "#             pickle.dump(classifier, f)\n",
    "#         print(f\"Saved XGBoost classifier for level {level}, fold {fold_idx} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stack all 5 folds' predictions and true labels together\n",
    "# all_true_labels = np.concatenate([_fold['true_labels'] for _fold in prec_rf_val_predictions[2]])\n",
    "# all_predicted_labels = np.concatenate([_fold['predicted_labels'] for _fold in prec_rf_val_predictions[2]])\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "# # Create a figure with larger size\n",
    "# plt.figure(figsize=(20, 16))\n",
    "\n",
    "# # Create heatmap\n",
    "# sns.heatmap(cm, \n",
    "#             xticklabels=np.unique(all_true_labels),\n",
    "#             yticklabels=np.unique(all_true_labels),\n",
    "#             annot=True, \n",
    "#             fmt='d',\n",
    "#             cmap='Blues')\n",
    "\n",
    "# plt.title('Confusion Matrix - Fold 0')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Print classification report for more detailed metrics\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(all_true_labels, all_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('random_forest_data.pkl', 'rb') as f:\n",
    "#    data = pickle.load(f)\n",
    "#prec_rf_classifiers = data['classifiers1']\n",
    "#prec_lgbm_classifiers = data['classifiers2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stack all 5 folds' predictions and true labels together\n",
    "# all_true_labels = np.concatenate([_fold['true_labels'] for _fold in prec_lgbm_val_predictions[2]])\n",
    "# all_predicted_labels = np.concatenate([_fold['predicted_labels'] for _fold in prec_lgbm_val_predictions[2]])\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "# # Create a figure with larger size\n",
    "# plt.figure(figsize=(20, 16))\n",
    "\n",
    "# # Create heatmap\n",
    "# sns.heatmap(cm, \n",
    "#             xticklabels=np.unique(all_true_labels),\n",
    "#             yticklabels=np.unique(all_true_labels),\n",
    "#             annot=True, \n",
    "#             fmt='d',\n",
    "#             cmap='Blues')\n",
    "\n",
    "# plt.title('Confusion Matrix - Fold 0')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Print classification report for more detailed metrics\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(all_true_labels, all_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = np.finfo(np.float32).max\n",
    "cliped_text_X = np.clip(test_X, -max_value, max_value)\n",
    "#cliped_text_X = np.clip(test_X, a_min=None, a_max=np.finfo(np.float32).max)\n",
    "#cliped_text_X = np.nan_to_num(test_X, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_Absolute energy</th>\n",
       "      <th>0_Area under the curve</th>\n",
       "      <th>0_Autocorrelation</th>\n",
       "      <th>0_Average power</th>\n",
       "      <th>0_Centroid</th>\n",
       "      <th>0_ECDF Percentile Count_0</th>\n",
       "      <th>0_ECDF Percentile Count_1</th>\n",
       "      <th>0_ECDF Percentile_0</th>\n",
       "      <th>0_ECDF Percentile_1</th>\n",
       "      <th>0_ECDF_0</th>\n",
       "      <th>...</th>\n",
       "      <th>time_diffmax</th>\n",
       "      <th>time_diffmin</th>\n",
       "      <th>time_diffmedian</th>\n",
       "      <th>time_diffstd</th>\n",
       "      <th>time_diffvar</th>\n",
       "      <th>time_burstiness</th>\n",
       "      <th>time_total</th>\n",
       "      <th>time_event_density</th>\n",
       "      <th>time_entropy</th>\n",
       "      <th>time_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.440782e+04</td>\n",
       "      <td>1.503495e+03</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2.512760e+02</td>\n",
       "      <td>350.938229</td>\n",
       "      <td>8.060000e+02</td>\n",
       "      <td>3.225000e+03</td>\n",
       "      <td>4.693100e+00</td>\n",
       "      <td>4.693100e+00</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>...</td>\n",
       "      <td>1800.804</td>\n",
       "      <td>44.334</td>\n",
       "      <td>300.0210</td>\n",
       "      <td>37.923387</td>\n",
       "      <td>1.438183e+03</td>\n",
       "      <td>-0.775550</td>\n",
       "      <td>1209300.493</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>11.969387</td>\n",
       "      <td>300.164008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.474633e+17</td>\n",
       "      <td>5.765358e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.097309e+14</td>\n",
       "      <td>671.841696</td>\n",
       "      <td>4.294968e+06</td>\n",
       "      <td>4.294968e+06</td>\n",
       "      <td>4.294968e+06</td>\n",
       "      <td>4.294968e+06</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>...</td>\n",
       "      <td>2795.428</td>\n",
       "      <td>432.977</td>\n",
       "      <td>599.9825</td>\n",
       "      <td>67.312288</td>\n",
       "      <td>4.530944e+03</td>\n",
       "      <td>-0.799636</td>\n",
       "      <td>4837908.512</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>12.960075</td>\n",
       "      <td>604.925698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.051532e+05</td>\n",
       "      <td>4.752845e+03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.211762e+03</td>\n",
       "      <td>167.236650</td>\n",
       "      <td>1.423650e+01</td>\n",
       "      <td>1.423650e+01</td>\n",
       "      <td>1.423650e+01</td>\n",
       "      <td>1.423650e+01</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>...</td>\n",
       "      <td>1257.186</td>\n",
       "      <td>474.635</td>\n",
       "      <td>599.6460</td>\n",
       "      <td>34.498242</td>\n",
       "      <td>1.190129e+03</td>\n",
       "      <td>-0.891520</td>\n",
       "      <td>1203661.837</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>10.964569</td>\n",
       "      <td>601.461213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.639011e+07</td>\n",
       "      <td>3.919227e+04</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>7.490392e+03</td>\n",
       "      <td>449.800951</td>\n",
       "      <td>1.464000e+03</td>\n",
       "      <td>5.857000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.471000e+01</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>...</td>\n",
       "      <td>380113.304</td>\n",
       "      <td>187.020</td>\n",
       "      <td>599.9890</td>\n",
       "      <td>4449.767580</td>\n",
       "      <td>1.980043e+07</td>\n",
       "      <td>0.741404</td>\n",
       "      <td>4837597.819</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>12.184404</td>\n",
       "      <td>671.192168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.044000e+03</td>\n",
       "      <td>6.717552e+02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.018561e+00</td>\n",
       "      <td>337.714842</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>...</td>\n",
       "      <td>1202.283</td>\n",
       "      <td>228.718</td>\n",
       "      <td>599.9110</td>\n",
       "      <td>32.974216</td>\n",
       "      <td>1.087299e+03</td>\n",
       "      <td>-0.895506</td>\n",
       "      <td>2418916.993</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>11.979174</td>\n",
       "      <td>598.376379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315715</th>\n",
       "      <td>5.784299e+07</td>\n",
       "      <td>3.735436e+04</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.722380e+05</td>\n",
       "      <td>166.327444</td>\n",
       "      <td>4.040000e+02</td>\n",
       "      <td>1.616000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.983040e+02</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>...</td>\n",
       "      <td>1187.473</td>\n",
       "      <td>179.672</td>\n",
       "      <td>599.9310</td>\n",
       "      <td>28.724209</td>\n",
       "      <td>8.250802e+02</td>\n",
       "      <td>-0.908410</td>\n",
       "      <td>1208994.487</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>10.978171</td>\n",
       "      <td>598.241785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315716</th>\n",
       "      <td>3.649651e+07</td>\n",
       "      <td>5.168745e+04</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.432153e+04</td>\n",
       "      <td>319.448996</td>\n",
       "      <td>8.050000e+02</td>\n",
       "      <td>3.220000e+03</td>\n",
       "      <td>3.396420e+01</td>\n",
       "      <td>1.113395e+02</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>...</td>\n",
       "      <td>2180.024</td>\n",
       "      <td>460.211</td>\n",
       "      <td>599.9415</td>\n",
       "      <td>38.317201</td>\n",
       "      <td>1.468208e+03</td>\n",
       "      <td>-0.880144</td>\n",
       "      <td>2418699.333</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>11.972423</td>\n",
       "      <td>600.968511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315717</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>...</td>\n",
       "      <td>76196.536</td>\n",
       "      <td>517.170</td>\n",
       "      <td>600.2930</td>\n",
       "      <td>1227.229968</td>\n",
       "      <td>1.506093e+06</td>\n",
       "      <td>0.320393</td>\n",
       "      <td>2418610.571</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>11.711933</td>\n",
       "      <td>619.810285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315718</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>...</td>\n",
       "      <td>1796.102</td>\n",
       "      <td>3.384</td>\n",
       "      <td>7.1840</td>\n",
       "      <td>297.064473</td>\n",
       "      <td>8.824730e+04</td>\n",
       "      <td>-0.002320</td>\n",
       "      <td>1209004.416</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>11.037622</td>\n",
       "      <td>298.463481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315719</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>...</td>\n",
       "      <td>1207.959</td>\n",
       "      <td>538.814</td>\n",
       "      <td>599.9590</td>\n",
       "      <td>21.262386</td>\n",
       "      <td>4.520891e+02</td>\n",
       "      <td>-0.931617</td>\n",
       "      <td>1209008.106</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>10.974389</td>\n",
       "      <td>600.627093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315720 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0_Absolute energy  0_Area under the curve  0_Autocorrelation  \\\n",
       "0            8.440782e+04            1.503495e+03              122.0   \n",
       "1            1.474633e+17            5.765358e+09                2.0   \n",
       "2            4.051532e+05            4.752845e+03                1.0   \n",
       "3            1.639011e+07            3.919227e+04             1014.0   \n",
       "4            4.044000e+03            6.717552e+02                1.0   \n",
       "...                   ...                     ...                ...   \n",
       "315715       5.784299e+07            3.735436e+04               31.0   \n",
       "315716       3.649651e+07            5.168745e+04               28.0   \n",
       "315717       0.000000e+00            0.000000e+00                1.0   \n",
       "315718       0.000000e+00            0.000000e+00                1.0   \n",
       "315719       0.000000e+00            0.000000e+00                1.0   \n",
       "\n",
       "        0_Average power  0_Centroid  0_ECDF Percentile Count_0  \\\n",
       "0          2.512760e+02  350.938229               8.060000e+02   \n",
       "1          1.097309e+14  671.841696               4.294968e+06   \n",
       "2          1.211762e+03  167.236650               1.423650e+01   \n",
       "3          7.490392e+03  449.800951               1.464000e+03   \n",
       "4          6.018561e+00  337.714842               1.000000e+00   \n",
       "...                 ...         ...                        ...   \n",
       "315715     1.722380e+05  166.327444               4.040000e+02   \n",
       "315716     5.432153e+04  319.448996               8.050000e+02   \n",
       "315717     0.000000e+00    0.000000               0.000000e+00   \n",
       "315718     0.000000e+00    0.000000               0.000000e+00   \n",
       "315719     0.000000e+00    0.000000               0.000000e+00   \n",
       "\n",
       "        0_ECDF Percentile Count_1  0_ECDF Percentile_0  0_ECDF Percentile_1  \\\n",
       "0                    3.225000e+03         4.693100e+00         4.693100e+00   \n",
       "1                    4.294968e+06         4.294968e+06         4.294968e+06   \n",
       "2                    1.423650e+01         1.423650e+01         1.423650e+01   \n",
       "3                    5.857000e+03         0.000000e+00         7.471000e+01   \n",
       "4                    1.000000e+00         1.000000e+00         1.000000e+00   \n",
       "...                           ...                  ...                  ...   \n",
       "315715               1.616000e+03         0.000000e+00         2.983040e+02   \n",
       "315716               3.220000e+03         3.396420e+01         1.113395e+02   \n",
       "315717               0.000000e+00         0.000000e+00         0.000000e+00   \n",
       "315718               0.000000e+00         0.000000e+00         0.000000e+00   \n",
       "315719               0.000000e+00         0.000000e+00         0.000000e+00   \n",
       "\n",
       "        0_ECDF_0  ...  time_diffmax  time_diffmin  time_diffmedian  \\\n",
       "0       0.000248  ...      1800.804        44.334         300.0210   \n",
       "1       0.000125  ...      2795.428       432.977         599.9825   \n",
       "2       0.000500  ...      1257.186       474.635         599.6460   \n",
       "3       0.000137  ...    380113.304       187.020         599.9890   \n",
       "4       0.000247  ...      1202.283       228.718         599.9110   \n",
       "...          ...  ...           ...           ...              ...   \n",
       "315715  0.000495  ...      1187.473       179.672         599.9310   \n",
       "315716  0.000248  ...      2180.024       460.211         599.9415   \n",
       "315717  0.000261  ...     76196.536       517.170         600.2930   \n",
       "315718  0.000247  ...      1796.102         3.384           7.1840   \n",
       "315719  0.000497  ...      1207.959       538.814         599.9590   \n",
       "\n",
       "        time_diffstd  time_diffvar  time_burstiness   time_total  \\\n",
       "0          37.923387  1.438183e+03        -0.775550  1209300.493   \n",
       "1          67.312288  4.530944e+03        -0.799636  4837908.512   \n",
       "2          34.498242  1.190129e+03        -0.891520  1203661.837   \n",
       "3        4449.767580  1.980043e+07         0.741404  4837597.819   \n",
       "4          32.974216  1.087299e+03        -0.895506  2418916.993   \n",
       "...              ...           ...              ...          ...   \n",
       "315715     28.724209  8.250802e+02        -0.908410  1208994.487   \n",
       "315716     38.317201  1.468208e+03        -0.880144  2418699.333   \n",
       "315717   1227.229968  1.506093e+06         0.320393  2418610.571   \n",
       "315718    297.064473  8.824730e+04        -0.002320  1209004.416   \n",
       "315719     21.262386  4.520891e+02        -0.931617  1209008.106   \n",
       "\n",
       "        time_event_density  time_entropy  time_slope  \n",
       "0                 0.003334     11.969387  300.164008  \n",
       "1                 0.001654     12.960075  604.925698  \n",
       "2                 0.001663     10.964569  601.461213  \n",
       "3                 0.001514     12.184404  671.192168  \n",
       "4                 0.001672     11.979174  598.376379  \n",
       "...                    ...           ...         ...  \n",
       "315715            0.001672     10.978171  598.241785  \n",
       "315716            0.001664     11.972423  600.968511  \n",
       "315717            0.001584     11.711933  619.810285  \n",
       "315718            0.003352     11.037622  298.463481  \n",
       "315719            0.001666     10.974389  600.627093  \n",
       "\n",
       "[315720 rows x 161 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cliped_text_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliped_text_X = cliped_text_X.replace([np.inf, -np.inf], 0.0)  # Replace infinity\n",
    "cliped_text_X = cliped_text_X.fillna(0.0)  # Replace NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN values: False\n",
      "Checking for infinite values: False\n",
      "Maximum value: 3.4028234663852886e+38\n",
      "Minimum value: -3.4028234663852886e+38\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for NaN values:\", np.any(np.isnan(cliped_text_X)))\n",
    "print(\"Checking for infinite values:\", np.any(np.isinf(cliped_text_X)))\n",
    "print(\"Maximum value:\", np.max(cliped_text_X))\n",
    "print(\"Minimum value:\", np.min(cliped_text_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='int64')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cliped_text_X[cliped_text_X.isnull().any(axis=1)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the downloaded classifiers\n",
    "\n",
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classifiers successfully loaded into `prec_rf_classifiers`.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Initialize the 2D list to store classifiers\n",
    "prec_rf_classifiers = []\n",
    "\n",
    "# Loop through each level\n",
    "for level in range(5):\n",
    "    level_classifiers = []  # List to store classifiers for the current level\n",
    "    \n",
    "    # Loop through each fold for the current level\n",
    "    for fold in range(10):\n",
    "        # Construct the filename\n",
    "        file_path = f'./rf-models-default-params/rf_classifier_level_{level}_fold_{fold}.pkl'\n",
    "        \n",
    "        # Load the classifier\n",
    "        with open(file_path, 'rb') as f:\n",
    "            classifier = pickle.load(f)\n",
    "        \n",
    "        # Add the classifier to the list for the current level\n",
    "        level_classifiers.append(classifier)\n",
    "    \n",
    "    # Append the list of classifiers for the current level to the main list\n",
    "    prec_rf_classifiers.append(level_classifiers)\n",
    "\n",
    "# The `prec_rf_classifiers` list now contains all classifiers organized by level and fold\n",
    "print(\"All classifiers successfully loaded into `prec_rf_classifiers`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prec_rf_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_rf_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classifiers successfully loaded into `prec_lgbm_classifiers`.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Initialize the 2D list to store classifiers\n",
    "prec_lgbm_classifiers = []\n",
    "\n",
    "# Loop through each level\n",
    "for level in range(5):\n",
    "    level_classifiers = []  # List to store classifiers for the current level\n",
    "    \n",
    "    # Loop through each fold for the current level\n",
    "    for fold in range(10):\n",
    "        # Construct the filename\n",
    "        if (level == 1) or (level == 2) or (level == 3):\n",
    "            level_classifiers.append(prec_rf_classifiers[level][fold])\n",
    "            continue\n",
    "        else:\n",
    "            file_path = f'./lgbm-models-default-params/lgbm_classifier_level_{level}_fold_{fold}.pkl'\n",
    "        \n",
    "        # Load the classifier\n",
    "        with open(file_path, 'rb') as f:\n",
    "            classifier = pickle.load(f)\n",
    "        \n",
    "        # Add the classifier to the list for the current level\n",
    "        level_classifiers.append(classifier)\n",
    "    \n",
    "    # Append the list of classifiers for the current level to the main list\n",
    "    prec_lgbm_classifiers.append(level_classifiers)\n",
    "\n",
    "# The `prec_rf_classifiers` list now contains all classifiers organized by level and fold\n",
    "print(\"All classifiers successfully loaded into `prec_lgbm_classifiers`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1)]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_lgbm_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prec_lgbm_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classifiers successfully loaded into `xgb_rf_classifiers`.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Initialize the 2D list to store classifiers\n",
    "prec_xgb_classifiers = []\n",
    "\n",
    "# Loop through each level\n",
    "for level in range(5):\n",
    "    level_classifiers = []  # List to store classifiers for the current level\n",
    "    \n",
    "    # Loop through each fold for the current level\n",
    "    for fold in range(10):\n",
    "        # Construct the filename\n",
    "        file_path = f'./xgb-models-400-estimators-0.3-lr/xgb_classifier_level_{level}_fold_{fold}.pkl'\n",
    "        \n",
    "        # Load the classifier\n",
    "        with open(file_path, 'rb') as f:\n",
    "            classifier = pickle.load(f)\n",
    "        \n",
    "        # Add the classifier to the list for the current level\n",
    "        level_classifiers.append(classifier)\n",
    "    \n",
    "    # Append the list of classifiers for the current level to the main list\n",
    "    prec_xgb_classifiers.append(level_classifiers)\n",
    "\n",
    "# The `prec_rf_classifiers` list now contains all classifiers organized by level and fold\n",
    "print(\"All classifiers successfully loaded into `xgb_rf_classifiers`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...)],\n",
       " [XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...)],\n",
       " [XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...)],\n",
       " [XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...)],\n",
       " [XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...)]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_xgb_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prec_xgb_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_with_models_vote(classifiers1, classifiers2, classifiers3, test_data):\n",
    "    \"\"\"\n",
    "    Make probability predictions using multiple classifier models\n",
    "    \n",
    "    Args:\n",
    "        classifiers: List of trained classifier models\n",
    "        test_data: Test data to make predictions on\n",
    "        \n",
    "    Returns:\n",
    "        List of probability predictions from each classifier\n",
    "    \"\"\"\n",
    "    test_preds_all = []\n",
    "    for i in tqdm(range(len(classifiers1))):\n",
    "    #for clf in tqdm(classifiers):\n",
    "        pred1 = classifiers1[i].predict_proba(test_data)\n",
    "        pred2 = classifiers2[i].predict_proba(test_data)\n",
    "        pred3 = classifiers3[i].predict_proba(test_data)\n",
    "        pred = (pred1+pred2+pred3)/3.0\n",
    "        test_preds_all.append(pred)\n",
    "    return test_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_combine_predictions(classifiers1, classifiers2, classifiers3, test_preds_all, test_data, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Aligns predictions from multiple classifiers and combines them through averaging\n",
    "    \n",
    "    Args:\n",
    "        classifiers: List of trained classifier models\n",
    "        test_preds_all: List of probability predictions from each classifier\n",
    "        test_data: Test data used for predictions\n",
    "        threshold: Minimum probability threshold for making predictions\n",
    "        \n",
    "    Returns:\n",
    "        Final class predictions after aligning and combining probabilities\n",
    "    \"\"\"\n",
    "    # Get the common classes across all classifiers\n",
    "    all_classes = classifiers2[0].classes_\n",
    "    test_preds_aligned = []\n",
    "\n",
    "    # Make predictions with each fold's model and align them \n",
    "    for i, clf in tqdm(enumerate(classifiers2)):\n",
    "        pred = test_preds_all[i]\n",
    "        # Create a mapping to align predictions with common classes\n",
    "        pred_dict = {_cls: idx for idx, _cls in enumerate(clf.classes_)}\n",
    "        aligned_pred = np.zeros((len(test_data), len(all_classes)))\n",
    "        \n",
    "        for i, _cls in enumerate(all_classes):\n",
    "            if _cls in pred_dict:\n",
    "                aligned_pred[:, i] = pred[:, pred_dict[_cls]]\n",
    "        \n",
    "        test_preds_aligned.append(aligned_pred)\n",
    "\n",
    "    # Stack and average the aligned predictions\n",
    "    test_preds_all = np.stack(test_preds_aligned)\n",
    "    test_preds_proba = test_preds_all.mean(axis=0)\n",
    "\n",
    "    # Get max probabilities for each prediction\n",
    "    max_probs = np.max(test_preds_proba, axis=1)\n",
    "    \n",
    "    # Convert probabilities to class predictions, using threshold\n",
    "    test_preds = np.array(['None'] * len(test_data), dtype=object)\n",
    "    confident_mask = max_probs >= threshold\n",
    "    test_preds[confident_mask] = all_classes[np.argmax(test_preds_proba[confident_mask], axis=1)]\n",
    "    \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_preds = []\n",
    "thr_list = [0.0, 0.2, 0.3, 0.4, 0.5]\n",
    "for i in range(5):\n",
    "    print(f\"Predicting level {i}\")\n",
    "    test_preds_all = make_predictions_with_models(prec_classifiers[i], cliped_text_X)\n",
    "    test_preds.append(align_and_combine_predictions(prec_classifiers[i], test_preds_all, cliped_text_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting level 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:39<00:00,  9.92s/it]\n",
      "10it [00:00, 56.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting level 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [05:26<00:00, 32.67s/it]\n",
      "10it [00:05,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting level 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [04:05<00:00, 24.54s/it]\n",
      "10it [00:02,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting level 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:30<00:00, 15.05s/it]\n",
      "10it [00:01,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting level 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:26<00:00,  8.69s/it]\n",
      "10it [00:00, 28.38it/s]\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "thr_list = [0.0, 0.2, 0.3, 0.4, 0.5]\n",
    "for i in range(5):\n",
    "    print(f\"Predicting level {i}\")\n",
    "    test_preds_all = make_predictions_with_models_vote(prec_rf_classifiers[i], prec_lgbm_classifiers[i], prec_xgb_classifiers[i], cliped_text_X)\n",
    "    test_preds.append(align_and_combine_predictions(prec_rf_classifiers[i], prec_lgbm_classifiers[i], prec_xgb_classifiers[i], test_preds_all, cliped_text_X))\n",
    "    #test_preds_all = make_predictions_with_models_vote(prec_rf_classifiers[i], prec_lgbm_classifiers[i], prec_xgb_classifiers[i], test_X)\n",
    "    #test_preds.append(align_and_combine_predictions(prec_rf_classifiers[i], prec_lgbm_classifiers[i], prec_xgb_classifiers[i], test_preds_all, test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.35642519e-03, 4.55702914e-06, 1.00267337e-02, 5.71873737e-06,\n",
       "       8.75914799e-01, 9.72787444e-02, 1.00679583e-02, 3.33763706e-03,\n",
       "       7.42273891e-06])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_all[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 315720/315720 [00:03<00:00, 83791.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['Sensor', 'Power_Sensor', 'Electrical_Power_Sensor', 'None',\n",
       "        'None'],\n",
       "       ['Sensor', 'Flow_Sensor', 'Chilled_Water_Supply_Flow_Sensor',\n",
       "        'Water_Flow_Sensor', 'None'],\n",
       "       ['Sensor', 'Temperature_Setpoint', 'Power_Sensor',\n",
       "        'Electrical_Power_Sensor', 'Peak_Power_Demand_Sensor'],\n",
       "       ...,\n",
       "       ['Sensor', 'None', 'None', 'None', 'None'],\n",
       "       ['Sensor', 'Power_Sensor', 'None', 'None', 'None'],\n",
       "       ['Alarm', 'None', 'None', 'None', 'None']], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to array and process None values\n",
    "stacked = np.stack(test_preds).transpose()\n",
    "for row in tqdm(stacked):\n",
    "    # Find first occurrence of 'None' if any\n",
    "    none_idx = np.where(row == 'None')[0]\n",
    "    if len(none_idx) > 0:\n",
    "        # Set all elements after first None to None\n",
    "        first_none = none_idx[0]\n",
    "        row[first_none:] = 'None'\n",
    "        \n",
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming `number_to_label` is already defined from the previous code\n",
    "# # Create a function to map numbers to labels\n",
    "# def map_numbers_to_labels(array, mapping):\n",
    "#     for row in tqdm(array):\n",
    "#         for i, value in enumerate(row):\n",
    "#             # If the value is numeric, map it to the label\n",
    "#             if isinstance(value, (int, np.integer)):\n",
    "#                 row[i] = mapping.get(value, value)  # Keep value if not in mapping\n",
    "\n",
    "# # Example mapping dictionary (you should replace this with your actual mapping)\n",
    "# number_to_label = {i: label for i, label in enumerate(train_y.columns[1:].tolist())}\n",
    "\n",
    "# # Apply the mapping to the array\n",
    "# map_numbers_to_labels(stacked, number_to_label)\n",
    "\n",
    "# # Output the processed array\n",
    "# stacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnlist = ['Active_Power_Sensor', 'Air_Flow_Sensor',\n",
    "       'Air_Flow_Setpoint', 'Air_Temperature_Sensor',\n",
    "       'Air_Temperature_Setpoint', 'Alarm', 'Angle_Sensor',\n",
    "       'Average_Zone_Air_Temperature_Sensor',\n",
    "       'Chilled_Water_Differential_Temperature_Sensor',\n",
    "       'Chilled_Water_Return_Temperature_Sensor',\n",
    "       'Chilled_Water_Supply_Flow_Sensor',\n",
    "       'Chilled_Water_Supply_Temperature_Sensor', 'Command',\n",
    "       'Cooling_Demand_Sensor', 'Cooling_Demand_Setpoint',\n",
    "       'Cooling_Supply_Air_Temperature_Deadband_Setpoint',\n",
    "       'Cooling_Temperature_Setpoint', 'Current_Sensor',\n",
    "       'Damper_Position_Sensor', 'Damper_Position_Setpoint', 'Demand_Sensor',\n",
    "       'Dew_Point_Setpoint', 'Differential_Pressure_Sensor',\n",
    "       'Differential_Pressure_Setpoint',\n",
    "       'Differential_Supply_Return_Water_Temperature_Sensor',\n",
    "       'Discharge_Air_Dewpoint_Sensor', 'Discharge_Air_Temperature_Sensor',\n",
    "       'Discharge_Air_Temperature_Setpoint',\n",
    "       'Discharge_Water_Temperature_Sensor', 'Duration_Sensor',\n",
    "       'Electrical_Power_Sensor', 'Energy_Usage_Sensor',\n",
    "       'Filter_Differential_Pressure_Sensor', 'Flow_Sensor', 'Flow_Setpoint',\n",
    "       'Frequency_Sensor', 'Heating_Demand_Sensor', 'Heating_Demand_Setpoint',\n",
    "       'Heating_Supply_Air_Temperature_Deadband_Setpoint',\n",
    "       'Heating_Temperature_Setpoint', 'Hot_Water_Flow_Sensor',\n",
    "       'Hot_Water_Return_Temperature_Sensor',\n",
    "       'Hot_Water_Supply_Temperature_Sensor', 'Humidity_Setpoint',\n",
    "       'Load_Current_Sensor', 'Low_Outside_Air_Temperature_Enable_Setpoint',\n",
    "       'Max_Air_Temperature_Setpoint', 'Min_Air_Temperature_Setpoint',\n",
    "       'Outside_Air_CO2_Sensor', 'Outside_Air_Enthalpy_Sensor',\n",
    "       'Outside_Air_Humidity_Sensor',\n",
    "       'Outside_Air_Lockout_Temperature_Setpoint',\n",
    "       'Outside_Air_Temperature_Sensor', 'Outside_Air_Temperature_Setpoint',\n",
    "       'Parameter', 'Peak_Power_Demand_Sensor', 'Position_Sensor',\n",
    "       'Power_Sensor', 'Pressure_Sensor', 'Rain_Sensor',\n",
    "       'Reactive_Power_Sensor', 'Reset_Setpoint',\n",
    "       'Return_Air_Temperature_Sensor', 'Return_Water_Temperature_Sensor',\n",
    "       'Room_Air_Temperature_Setpoint', 'Sensor', 'Setpoint',\n",
    "       'Solar_Radiance_Sensor', 'Speed_Setpoint', 'Static_Pressure_Sensor',\n",
    "       'Static_Pressure_Setpoint', 'Status', 'Supply_Air_Humidity_Sensor',\n",
    "       'Supply_Air_Static_Pressure_Sensor',\n",
    "       'Supply_Air_Static_Pressure_Setpoint', 'Supply_Air_Temperature_Sensor',\n",
    "       'Supply_Air_Temperature_Setpoint', 'Temperature_Sensor',\n",
    "       'Temperature_Setpoint', 'Thermal_Power_Sensor', 'Time_Setpoint',\n",
    "       'Usage_Sensor', 'Valve_Position_Sensor', 'Voltage_Sensor',\n",
    "       'Warmest_Zone_Air_Temperature_Sensor', 'Water_Flow_Sensor',\n",
    "       'Water_Temperature_Sensor', 'Water_Temperature_Setpoint',\n",
    "       'Wind_Direction_Sensor', 'Wind_Speed_Sensor',\n",
    "       'Zone_Air_Dewpoint_Sensor', 'Zone_Air_Humidity_Sensor',\n",
    "       'Zone_Air_Humidity_Setpoint', 'Zone_Air_Temperature_Sensor'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipftest = ZipFile('./test_X_v0.1.0.zip', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "listtestfile = zipftest.namelist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 315720/315720 [03:51<00:00, 1363.27it/s]\n"
     ]
    }
   ],
   "source": [
    "stackedfinalresult = pd.DataFrame(columns=['filename'])\n",
    "stackedfinalresult['filename'] = pd.Series(listtestfile).apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "for labelname in columnlist:\n",
    "    stackedfinalresult[labelname] = 0\n",
    "\n",
    "test_preds = stacked\n",
    "for i in tqdm(range(len(test_preds))):\n",
    "    # stackedfinalresult.loc[i, test_preds[i]] = 1\n",
    "    predlist = test_preds[i].tolist()\n",
    "    predlist = [x for x in predlist if x != 'None']\n",
    "    for predlabelname in predlist:\n",
    "    \tstackedfinalresult.loc[i, predlabelname] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedfinalresult = stackedfinalresult.assign(**{col: stackedfinalresult[col].astype(float) for col in stackedfinalresult.columns if col != \"filename\"})\n",
    "stackedfinalresult.to_csv(\"13-01.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
