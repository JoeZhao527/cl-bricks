{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "import warnings\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tsfel\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from hiclass import LocalClassifierPerNode, LocalClassifierPerParentNode, LocalClassifierPerLevel\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(train_y, n_splits=10):\n",
    "    # Create a label array for stratification\n",
    "    # We'll use the first non-zero label for each row as the stratification target\n",
    "    stratify_labels = []\n",
    "    for _, row in train_y.iterrows():\n",
    "        labels = row[train_y.columns != 'filename'].values\n",
    "        # Get first non-negative label, or 0 if all negative\n",
    "        first_positive = next((i for i, x in enumerate(labels) if x >= 0), 0)\n",
    "        stratify_labels.append(first_positive)\n",
    "    \n",
    "    # Create StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Generate fold indices\n",
    "    folds = []\n",
    "    for train_idx, val_idx in skf.split(train_y, stratify_labels):\n",
    "        folds.append({\n",
    "            'train': train_idx,\n",
    "            'val': val_idx\n",
    "        })\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['0_Absolute energy',\n",
    " '0_Area under the curve',\n",
    " '0_Autocorrelation',\n",
    " '0_Average power',\n",
    " '0_Centroid',\n",
    " '0_ECDF Percentile Count_0',\n",
    " '0_ECDF Percentile Count_1',\n",
    " '0_ECDF Percentile_0',\n",
    " '0_ECDF Percentile_1',\n",
    " '0_ECDF_0',\n",
    " '0_ECDF_1',\n",
    " '0_ECDF_2',\n",
    " '0_ECDF_3',\n",
    " '0_ECDF_4',\n",
    " '0_ECDF_5',\n",
    " '0_ECDF_6',\n",
    " '0_ECDF_7',\n",
    " '0_ECDF_8',\n",
    " '0_ECDF_9',\n",
    " '0_Entropy',\n",
    " '0_Histogram mode',\n",
    " '0_Interquartile range',\n",
    " '0_Kurtosis',\n",
    " '0_Max',\n",
    " '0_Mean',\n",
    " '0_Mean absolute deviation',\n",
    " '0_Mean absolute diff',\n",
    " '0_Mean diff',\n",
    " '0_Median',\n",
    " '0_Median absolute deviation',\n",
    " '0_Median absolute diff',\n",
    " '0_Median diff',\n",
    " '0_Min',\n",
    " '0_Negative turning points',\n",
    " '0_Neighbourhood peaks',\n",
    " '0_Peak to peak distance',\n",
    " '0_Positive turning points',\n",
    " '0_Root mean square',\n",
    " '0_Signal distance',\n",
    " '0_Skewness',\n",
    " '0_Slope',\n",
    " '0_Standard deviation',\n",
    " '0_Sum absolute diff',\n",
    " '0_Variance',\n",
    " '0_Zero crossing rate',\n",
    " '0_Fundamental frequency',\n",
    " '0_Human range energy',\n",
    " '0_Max power spectrum',\n",
    " '0_Maximum frequency',\n",
    " '0_Median frequency',\n",
    " '0_Power bandwidth',\n",
    " '0_Wavelet entropy',\n",
    " 'value_median',\n",
    " 'value_mean',\n",
    " 'value_qmean',\n",
    " 'value_max',\n",
    " 'value_min',\n",
    " 'value_maxmin',\n",
    " 'value_diffmax',\n",
    " 'value_diffmin',\n",
    " 'value_diffmean',\n",
    " 'value_diffqmean',\n",
    " 'value_diffmedian',\n",
    " 'value_diffmaxmin',\n",
    " 'time_diffmean',\n",
    " 'time_diffqmean',\n",
    " 'time_diffmax',\n",
    " 'time_diffmin',\n",
    " 'time_diffmedian',\n",
    " 'value_std',\n",
    " 'value_var',\n",
    " 'value_diffstd',\n",
    " 'value_diffvar',\n",
    " 'time_diffstd',\n",
    " 'time_diffvar',\n",
    " 'time_burstiness',\n",
    " 'time_total',\n",
    " 'time_event_density',\n",
    " 'time_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combineaugmented\n",
    "train_feat_dir = \"../downloads/train_data_features_v3_fixed\"\n",
    "\n",
    "train_0 = pd.read_csv(f\"{train_feat_dir}/train_features_full_v3.csv\", index_col=0)#[feature_list]\n",
    "feature_list = train_0.columns.tolist()\n",
    "feature_list = [item for item in feature_list if \"LPCC\" not in item]\n",
    "feature_list = [item for item in feature_list if \"MFCC\" not in item]\n",
    "\n",
    "train_0 = train_0[feature_list]\n",
    "train_1 = pd.read_csv(f\"{train_feat_dir}/train_features_split1_2_v3.csv\")[feature_list]\n",
    "train_2 = pd.read_csv(f\"{train_feat_dir}/train_features_split2_2_v3.csv\")[feature_list]\n",
    "train_3 = pd.read_csv(f\"{train_feat_dir}/train_features_split1_3_v3.csv\")[feature_list]\n",
    "train_4 = pd.read_csv(f\"{train_feat_dir}/train_features_split2_3_v3.csv\")[feature_list]\n",
    "train_5 = pd.read_csv(f\"{train_feat_dir}/train_features_split3_3_v3.csv\")[feature_list]\n",
    "train_6 = pd.read_csv(f\"{train_feat_dir}/train_features_split1_4_v3.csv\")[feature_list]\n",
    "train_7 = pd.read_csv(f\"{train_feat_dir}/train_features_split2_4_v3.csv\")[feature_list]\n",
    "train_8 = pd.read_csv(f\"{train_feat_dir}/train_features_split3_4_v3.csv\")[feature_list]\n",
    "train_9 = pd.read_csv(f\"{train_feat_dir}/train_features_split4_4_v3.csv\")[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"../downloads/train_y_v0.1.0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post process it dont forget!!!!\n",
    "train_y.loc[((train_y['Peak_Power_Demand_Sensor'] == 1)), \n",
    "            'Demand_Sensor'] = -1\n",
    "\n",
    "#train_y.loc[((train_y['Energy_Usage_Sensor'] == 1)), \n",
    "#            'Usage_Sensor'] = -1\n",
    "\n",
    "#'Setpoint', 'Temperature_Setpoint', 'Air_Temperature_Setpoint', 'Cooling_Temperature_Setpoint', 'Cooling_Supply_Air_Temperature_Deadband_Setpoint'\n",
    "train_y.loc[((train_y['Cooling_Supply_Air_Temperature_Deadband_Setpoint'] == 1)), \n",
    "            'Air_Temperature_Setpoint'] = -1\n",
    "train_y.loc[((train_y['Heating_Supply_Air_Temperature_Deadband_Setpoint'] == 1)), \n",
    "            'Air_Temperature_Setpoint'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.concat([train_y] * 10, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_Absolute energy</th>\n",
       "      <th>0_Area under the curve</th>\n",
       "      <th>0_Autocorrelation</th>\n",
       "      <th>0_Average power</th>\n",
       "      <th>0_Centroid</th>\n",
       "      <th>0_ECDF Percentile Count_0</th>\n",
       "      <th>0_ECDF Percentile Count_1</th>\n",
       "      <th>0_ECDF Percentile_0</th>\n",
       "      <th>0_ECDF Percentile_1</th>\n",
       "      <th>0_ECDF_0</th>\n",
       "      <th>...</th>\n",
       "      <th>time_diffmax</th>\n",
       "      <th>time_diffmin</th>\n",
       "      <th>time_diffmedian</th>\n",
       "      <th>time_diffstd</th>\n",
       "      <th>time_diffvar</th>\n",
       "      <th>time_burstiness</th>\n",
       "      <th>time_total</th>\n",
       "      <th>time_event_density</th>\n",
       "      <th>time_entropy</th>\n",
       "      <th>time_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.579300e+06</td>\n",
       "      <td>2.002655e+04</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.326999e+03</td>\n",
       "      <td>336.834861</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>...</td>\n",
       "      <td>29709.534</td>\n",
       "      <td>57.065</td>\n",
       "      <td>599.9990</td>\n",
       "      <td>461.649413</td>\n",
       "      <td>2.131202e+05</td>\n",
       "      <td>-0.133925</td>\n",
       "      <td>2418900.499</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>11.910432</td>\n",
       "      <td>605.860232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.051012e+06</td>\n",
       "      <td>3.009676e+04</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.014589e+03</td>\n",
       "      <td>672.251192</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.50</td>\n",
       "      <td>22.50</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>...</td>\n",
       "      <td>29705.420</td>\n",
       "      <td>35.112</td>\n",
       "      <td>600.0000</td>\n",
       "      <td>352.215834</td>\n",
       "      <td>1.240560e+05</td>\n",
       "      <td>-0.262351</td>\n",
       "      <td>4837688.953</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>12.930601</td>\n",
       "      <td>603.250804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.988878e+07</td>\n",
       "      <td>6.261729e+04</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.040152e+05</td>\n",
       "      <td>371.619154</td>\n",
       "      <td>805.0</td>\n",
       "      <td>3220.0</td>\n",
       "      <td>31.36</td>\n",
       "      <td>131.52</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>...</td>\n",
       "      <td>7200.115</td>\n",
       "      <td>279.083</td>\n",
       "      <td>600.0275</td>\n",
       "      <td>108.255439</td>\n",
       "      <td>1.171924e+04</td>\n",
       "      <td>-0.694783</td>\n",
       "      <td>2418874.283</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>11.965989</td>\n",
       "      <td>601.617578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.183314e+11</td>\n",
       "      <td>5.159254e+06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.805016e+07</td>\n",
       "      <td>706.907352</td>\n",
       "      <td>3834.0</td>\n",
       "      <td>3834.0</td>\n",
       "      <td>3834.00</td>\n",
       "      <td>3834.00</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>...</td>\n",
       "      <td>7770.911</td>\n",
       "      <td>285.595</td>\n",
       "      <td>600.0135</td>\n",
       "      <td>85.323742</td>\n",
       "      <td>7.280141e+03</td>\n",
       "      <td>-0.751199</td>\n",
       "      <td>4838073.366</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>12.970361</td>\n",
       "      <td>600.376320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.404025e+08</td>\n",
       "      <td>3.141691e+05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.292679e+05</td>\n",
       "      <td>672.200909</td>\n",
       "      <td>234.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>234.00</td>\n",
       "      <td>234.00</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>...</td>\n",
       "      <td>4802.769</td>\n",
       "      <td>305.036</td>\n",
       "      <td>599.9700</td>\n",
       "      <td>51.713450</td>\n",
       "      <td>2.674281e+03</td>\n",
       "      <td>-0.841547</td>\n",
       "      <td>4837577.658</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>12.972059</td>\n",
       "      <td>600.985625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318385</th>\n",
       "      <td>9.928028e+16</td>\n",
       "      <td>1.661917e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.927706e+14</td>\n",
       "      <td>85.590344</td>\n",
       "      <td>201.0</td>\n",
       "      <td>804.0</td>\n",
       "      <td>9760520.00</td>\n",
       "      <td>10122320.00</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>...</td>\n",
       "      <td>1180.751</td>\n",
       "      <td>304.968</td>\n",
       "      <td>600.0490</td>\n",
       "      <td>23.566671</td>\n",
       "      <td>5.553880e+02</td>\n",
       "      <td>-0.924407</td>\n",
       "      <td>602946.628</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>9.971950</td>\n",
       "      <td>599.315545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318386</th>\n",
       "      <td>1.002000e+03</td>\n",
       "      <td>1.671441e+02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.970967e+00</td>\n",
       "      <td>84.170978</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>...</td>\n",
       "      <td>1258.058</td>\n",
       "      <td>473.015</td>\n",
       "      <td>600.0940</td>\n",
       "      <td>31.148107</td>\n",
       "      <td>9.702045e+02</td>\n",
       "      <td>-0.901471</td>\n",
       "      <td>604123.302</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>9.971447</td>\n",
       "      <td>600.586043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318387</th>\n",
       "      <td>5.050000e+06</td>\n",
       "      <td>8.366623e+03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.035888e+04</td>\n",
       "      <td>41.833115</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>...</td>\n",
       "      <td>639.331</td>\n",
       "      <td>288.833</td>\n",
       "      <td>599.8335</td>\n",
       "      <td>26.006854</td>\n",
       "      <td>6.763564e+02</td>\n",
       "      <td>-0.916594</td>\n",
       "      <td>301198.426</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>8.975652</td>\n",
       "      <td>599.109134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318388</th>\n",
       "      <td>4.581000e+07</td>\n",
       "      <td>2.524991e+04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.442792e+05</td>\n",
       "      <td>42.083178</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.00</td>\n",
       "      <td>300.00</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>...</td>\n",
       "      <td>607.430</td>\n",
       "      <td>57.118</td>\n",
       "      <td>599.9915</td>\n",
       "      <td>41.760417</td>\n",
       "      <td>1.743932e+03</td>\n",
       "      <td>-0.869134</td>\n",
       "      <td>302998.882</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>8.982900</td>\n",
       "      <td>595.866076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318389</th>\n",
       "      <td>2.245000e+04</td>\n",
       "      <td>1.114173e+03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.007474e+02</td>\n",
       "      <td>111.417309</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>...</td>\n",
       "      <td>264604.898</td>\n",
       "      <td>596.383</td>\n",
       "      <td>599.9980</td>\n",
       "      <td>8814.866995</td>\n",
       "      <td>7.770188e+07</td>\n",
       "      <td>0.815779</td>\n",
       "      <td>802204.625</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>7.487196</td>\n",
       "      <td>1035.736916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318390 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0_Absolute energy  0_Area under the curve  0_Autocorrelation   \n",
       "0            3.579300e+06            2.002655e+04               17.0  \\\n",
       "1            4.051012e+06            3.009676e+04               16.0   \n",
       "2            6.988878e+07            6.261729e+04               32.0   \n",
       "3            1.183314e+11            5.159254e+06                1.0   \n",
       "4            4.404025e+08            3.141691e+05                2.0   \n",
       "...                   ...                     ...                ...   \n",
       "318385       9.928028e+16            1.661917e+09                2.0   \n",
       "318386       1.002000e+03            1.671441e+02                2.0   \n",
       "318387       5.050000e+06            8.366623e+03                1.0   \n",
       "318388       4.581000e+07            2.524991e+04                1.0   \n",
       "318389       2.245000e+04            1.114173e+03                1.0   \n",
       "\n",
       "        0_Average power  0_Centroid  0_ECDF Percentile Count_0   \n",
       "0          5.326999e+03  336.834861                       30.0  \\\n",
       "1          3.014589e+03  672.251192                       22.5   \n",
       "2          1.040152e+05  371.619154                      805.0   \n",
       "3          8.805016e+07  706.907352                     3834.0   \n",
       "4          3.292679e+05  672.200909                      234.0   \n",
       "...                 ...         ...                        ...   \n",
       "318385     5.927706e+14   85.590344                      201.0   \n",
       "318386     5.970967e+00   84.170978                        1.0   \n",
       "318387     6.035888e+04   41.833115                      100.0   \n",
       "318388     5.442792e+05   42.083178                      300.0   \n",
       "318389     1.007474e+02  111.417309                        5.0   \n",
       "\n",
       "        0_ECDF Percentile Count_1  0_ECDF Percentile_0  0_ECDF Percentile_1   \n",
       "0                            30.0                30.00                30.00  \\\n",
       "1                            22.5                22.50                22.50   \n",
       "2                          3220.0                31.36               131.52   \n",
       "3                          3834.0              3834.00              3834.00   \n",
       "4                           234.0               234.00               234.00   \n",
       "...                           ...                  ...                  ...   \n",
       "318385                      804.0           9760520.00          10122320.00   \n",
       "318386                        1.0                 1.00                 1.00   \n",
       "318387                      100.0               100.00               100.00   \n",
       "318388                      300.0               300.00               300.00   \n",
       "318389                        5.0                 5.00                 5.00   \n",
       "\n",
       "        0_ECDF_0  ...  time_diffmax  time_diffmin  time_diffmedian   \n",
       "0       0.000250  ...     29709.534        57.065         599.9990  \\\n",
       "1       0.000125  ...     29705.420        35.112         600.0000   \n",
       "2       0.000248  ...      7200.115       279.083         600.0275   \n",
       "3       0.000124  ...      7770.911       285.595         600.0135   \n",
       "4       0.000124  ...      4802.769       305.036         599.9700   \n",
       "...          ...  ...           ...           ...              ...   \n",
       "318385  0.000994  ...      1180.751       304.968         600.0490   \n",
       "318386  0.000994  ...      1258.058       473.015         600.0940   \n",
       "318387  0.001980  ...       639.331       288.833         599.8335   \n",
       "318388  0.001965  ...       607.430        57.118         599.9915   \n",
       "318389  0.001114  ...    264604.898       596.383         599.9980   \n",
       "\n",
       "        time_diffstd  time_diffvar  time_burstiness   time_total   \n",
       "0         461.649413  2.131202e+05        -0.133925  2418900.499  \\\n",
       "1         352.215834  1.240560e+05        -0.262351  4837688.953   \n",
       "2         108.255439  1.171924e+04        -0.694783  2418874.283   \n",
       "3          85.323742  7.280141e+03        -0.751199  4838073.366   \n",
       "4          51.713450  2.674281e+03        -0.841547  4837577.658   \n",
       "...              ...           ...              ...          ...   \n",
       "318385     23.566671  5.553880e+02        -0.924407   602946.628   \n",
       "318386     31.148107  9.702045e+02        -0.901471   604123.302   \n",
       "318387     26.006854  6.763564e+02        -0.916594   301198.426   \n",
       "318388     41.760417  1.743932e+03        -0.869134   302998.882   \n",
       "318389   8814.866995  7.770188e+07         0.815779   802204.625   \n",
       "\n",
       "        time_event_density  time_entropy   time_slope  \n",
       "0                 0.001655     11.910432   605.860232  \n",
       "1                 0.001659     12.930601   603.250804  \n",
       "2                 0.001664     11.965989   601.617578  \n",
       "3                 0.001665     12.970361   600.376320  \n",
       "4                 0.001664     12.972059   600.985625  \n",
       "...                    ...           ...          ...  \n",
       "318385            0.001668      9.971950   599.315545  \n",
       "318386            0.001665      9.971447   600.586043  \n",
       "318387            0.001677      8.975652   599.109134  \n",
       "318388            0.001680      8.982900   595.866076  \n",
       "318389            0.001119      7.487196  1035.736916  \n",
       "\n",
       "[318390 rows x 161 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = pd.concat([train_0, train_1, train_2, train_3, \n",
    "                     train_4, train_5, train_6, train_7, \n",
    "                     train_8, train_9], ignore_index=True)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_0, train_1, train_2, train_3, train_4, train_5, train_6, train_7, train_8, train_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_labels_np(row):\n",
    "    \"\"\"More efficient version using numpy\"\"\"\n",
    "    arr = row.to_numpy() # convert to numpy array\n",
    "    indices = np.where(arr == 1)[0] # get indices where value is 1\n",
    "    labels = row.index[indices].tolist() # get labels from indices\n",
    "    return labels\n",
    "\n",
    "labelhir = train_y.apply(get_active_labels_np, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a tier dict\n",
    "ontology_list = list(train_y.columns[1:])\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_tree(onto):\n",
    "    \"\"\"\n",
    "    Build a tree so that each term has at most one parent.\n",
    "    The parent is determined by the longest existing term that is a substring of the child.\n",
    "    \"\"\"\n",
    "    # Sort terms by length so that broader terms are processed (and assigned) first\n",
    "    sorted_onto = sorted(onto, key=len)\n",
    "    \n",
    "    # Dictionaries for storing parent-child relationships\n",
    "    parent_map = {}             # term -> parent\n",
    "    children_map = defaultdict(list)  # parent -> [children]\n",
    "\n",
    "    processed = []\n",
    "    \n",
    "    for term in sorted_onto:\n",
    "        # Find all processed terms that are substrings of 'term'\n",
    "        potential_parents = [p for p in processed if p in term]\n",
    "        \n",
    "        if not potential_parents:\n",
    "            # No parent found; this term is at the root\n",
    "            parent_map[term] = None\n",
    "            children_map[None].append(term)\n",
    "        else:\n",
    "            # Pick the longest parent (closest match)\n",
    "            parent = max(potential_parents, key=len)\n",
    "            parent_map[term] = parent\n",
    "            children_map[parent].append(term)\n",
    "        \n",
    "        processed.append(term)\n",
    "    \n",
    "    return parent_map, children_map\n",
    "\n",
    "\"\"\"\n",
    "Re-built hierachical labels\n",
    "\"\"\"\n",
    "level_labels = [[], [], [], [], []]\n",
    "\n",
    "def print_tree(children_map, root=None, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively print the tree structure with indentation.\n",
    "    'root=None' means we are listing top-level (root) terms first.\n",
    "    \"\"\"\n",
    "    \n",
    "    if root is None:\n",
    "        # For all top-level terms\n",
    "        for child in sorted(children_map[root]):\n",
    "            print_tree(children_map, child, depth)\n",
    "    else:\n",
    "        # print(\"  \" * depth + root)\n",
    "        level_labels[depth].append(root)\n",
    "        for child in sorted(children_map[root]):\n",
    "            print_tree(children_map, child, depth + 1)\n",
    "\n",
    "parent_map, children_map = build_tree(ontology_list)\n",
    "print_tree(children_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiers = {\n",
    "    1: level_labels[0],\n",
    "    2: level_labels[1],\n",
    "    3: level_labels[2],\n",
    "    4: level_labels[3],\n",
    "    5: level_labels[4]\n",
    "}\n",
    "\n",
    "def get_tier(label):\n",
    "    for tier_num, tier_list in tiers.items():\n",
    "        if label in tier_list:\n",
    "            return tier_num\n",
    "    return None  # Handle cases where the label isn't found in any tier\n",
    "\n",
    "def sort_labels(labels):\n",
    "    return sorted(labels, key=lambda label: (get_tier(label) or float('inf'), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_labelhir = [sort_labels(labels) for labels in labelhir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_hier = np.array(\n",
    "    sorted_labelhir,\n",
    "    dtype=object,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318390"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_label = np.array([x[-1] for x in label_hier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318390"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_label = pd.Series(label_hier).apply(lambda x: x + ['None'] * (5 - len(x)) if len(x) < 5 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318390"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tier0</th>\n",
       "      <th>tier1</th>\n",
       "      <th>tier2</th>\n",
       "      <th>tier3</th>\n",
       "      <th>tier4</th>\n",
       "      <th>combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ParameterNoneNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Setpoint</td>\n",
       "      <td>Temperature_Setpoint</td>\n",
       "      <td>Cooling_Temperature_Setpoint</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SetpointTemperature_SetpointCooling_Temperatur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Current_Sensor</td>\n",
       "      <td>Load_Current_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorCurrent_SensorLoad_Current_SensorNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Power_Sensor</td>\n",
       "      <td>Electrical_Power_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorPower_SensorElectrical_Power_SensorNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Setpoint</td>\n",
       "      <td>Speed_Setpoint</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SetpointSpeed_SetpointNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6699</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Power_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorPower_SensorNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>7929</td>\n",
       "      <td>Setpoint</td>\n",
       "      <td>Cooling_Demand_Setpoint</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SetpointCooling_Demand_SetpointNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>10899</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Temperature_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorTemperature_SensorNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>13579</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Flow_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorFlow_SensorNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>14098</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Outside_Air_Enthalpy_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorOutside_Air_Enthalpy_SensorNoneNoneNone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index      tier0                        tier1   \n",
       "0       0  Parameter                         None  \\\n",
       "1       1   Setpoint         Temperature_Setpoint   \n",
       "2       2     Sensor               Current_Sensor   \n",
       "3       3     Sensor                 Power_Sensor   \n",
       "4       4   Setpoint               Speed_Setpoint   \n",
       "..    ...        ...                          ...   \n",
       "86   6699     Sensor                 Power_Sensor   \n",
       "87   7929   Setpoint      Cooling_Demand_Setpoint   \n",
       "88  10899     Sensor           Temperature_Sensor   \n",
       "89  13579     Sensor                  Flow_Sensor   \n",
       "90  14098     Sensor  Outside_Air_Enthalpy_Sensor   \n",
       "\n",
       "                           tier2 tier3 tier4   \n",
       "0                           None  None  None  \\\n",
       "1   Cooling_Temperature_Setpoint  None  None   \n",
       "2            Load_Current_Sensor  None  None   \n",
       "3        Electrical_Power_Sensor  None  None   \n",
       "4                           None  None  None   \n",
       "..                           ...   ...   ...   \n",
       "86                          None  None  None   \n",
       "87                          None  None  None   \n",
       "88                          None  None  None   \n",
       "89                          None  None  None   \n",
       "90                          None  None  None   \n",
       "\n",
       "                                              combine  \n",
       "0                           ParameterNoneNoneNoneNone  \n",
       "1   SetpointTemperature_SetpointCooling_Temperatur...  \n",
       "2     SensorCurrent_SensorLoad_Current_SensorNoneNone  \n",
       "3   SensorPower_SensorElectrical_Power_SensorNoneNone  \n",
       "4                  SetpointSpeed_SetpointNoneNoneNone  \n",
       "..                                                ...  \n",
       "86                     SensorPower_SensorNoneNoneNone  \n",
       "87        SetpointCooling_Demand_SetpointNoneNoneNone  \n",
       "88               SensorTemperature_SensorNoneNoneNone  \n",
       "89                      SensorFlow_SensorNoneNoneNone  \n",
       "90      SensorOutside_Air_Enthalpy_SensorNoneNoneNone  \n",
       "\n",
       "[91 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listlabl = [] \n",
    "for i in range(len(padded_label)):\n",
    "    tierli = padded_label[i].copy()\n",
    "    tierli.append(''.join(tierli))\n",
    "    listlabl.append(tierli)\n",
    "pdlistlabl = pd.DataFrame(listlabl, columns=['tier0', 'tier1', 'tier2', 'tier3', 'tier4', 'combine'])\n",
    "pdlistlabl = pdlistlabl.drop_duplicates(subset=\"combine\").reset_index()\n",
    "pdlistlabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1: 0 None values out of 318390 total (0.00%)\n",
      "Level 2: 123210 None values out of 318390 total (38.70%)\n",
      "Level 3: 202470 None values out of 318390 total (63.59%)\n",
      "Level 4: 272160 None values out of 318390 total (85.48%)\n",
      "Level 5: 314700 None values out of 318390 total (98.84%)\n"
     ]
    }
   ],
   "source": [
    "# Count Nones at each level\n",
    "for i in range(5):\n",
    "    none_count = sum(padded_label.apply(lambda x: x[i] == 'None'))\n",
    "    print(f\"Level {i+1}: {none_count} None values out of {len(padded_label)} total ({none_count/len(padded_label):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(train_X, _label, folds, drop_none=False):\n",
    "    \"\"\"\n",
    "    Train random forest models using k-fold cross validation\n",
    "    \n",
    "    Args:\n",
    "        train_X: Training features DataFrame\n",
    "        _label: Array of labels\n",
    "        folds: List of dictionaries containing train/val indices\n",
    "        drop_none: Whether to drop samples with \"None\" labels\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (list of trained classifiers, list of scores, list of validation predictions)\n",
    "    \"\"\"\n",
    "    classifiers = []\n",
    "    scores = []\n",
    "    val_predictions = []  # List to store validation predictions\n",
    "\n",
    "    # Define Random Forest parameters\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': 8  # Use all available cores\n",
    "    }\n",
    "\n",
    "    for f_idx, fold in enumerate(folds):\n",
    "        # Prepare train and validation data for this fold\n",
    "        train_X_fold = train_X.iloc[fold['train']]\n",
    "        train_y_fold = _label[fold['train']]\n",
    "        val_X_fold = train_X.iloc[fold['val']]\n",
    "        val_y_fold = _label[fold['val']]\n",
    "        \n",
    "        if drop_none:\n",
    "            # Remove samples with \"None\" labels from training set\n",
    "            train_mask = train_y_fold != \"None\"\n",
    "            train_X_fold = train_X_fold[train_mask]\n",
    "            train_y_fold = train_y_fold[train_mask]\n",
    "            \n",
    "            # Remove samples with \"None\" labels from validation set\n",
    "            val_mask = val_y_fold != \"None\"\n",
    "            val_X_fold = val_X_fold[val_mask]\n",
    "            val_y_fold = val_y_fold[val_mask]\n",
    "            print(f\"Dropped train: {len(train_X_fold) - sum(train_mask)}, val: {len(val_X_fold) - sum(val_mask)}\")\n",
    "        \n",
    "        # Check the train_y_fold. If more than 30% of samples are labeled \"None\",\n",
    "        # randomly sample from the \"None\" to make that ratio no more than 30%.\n",
    "        none_mask = (train_y_fold == \"None\")\n",
    "        none_count = np.sum(none_mask)\n",
    "        total_samples = len(train_y_fold)\n",
    "        none_ratio = none_count / total_samples if total_samples > 0 else 0\n",
    "\n",
    "        if none_ratio > 0.4:\n",
    "            # Calculate how many \"None\" labels we should keep (30% of total)\n",
    "            max_none_to_keep = int(0.4 * (total_samples - none_count))\n",
    "\n",
    "            # Randomly choose which \"None\" labels to keep\n",
    "            none_indices = np.where(none_mask)[0]\n",
    "\n",
    "            # Fix the random seed before shuffling for reproducibility\n",
    "            rng = np.random.RandomState(f_idx)\n",
    "            rng.shuffle(none_indices)\n",
    "            \n",
    "            keep_none_indices = none_indices[:max_none_to_keep]\n",
    "\n",
    "            # Indices of all non-\"None\" labels\n",
    "            other_indices = np.where(~none_mask)[0]\n",
    "\n",
    "            # Combine indices to keep and then sort\n",
    "            new_indices = np.concatenate([keep_none_indices, other_indices])\n",
    "            new_indices = np.sort(new_indices)  # Sort so we can index the DataFrame consistently\n",
    "\n",
    "            # Subset the training data\n",
    "            train_X_fold = train_X_fold.iloc[new_indices]\n",
    "            train_y_fold = train_y_fold[new_indices]\n",
    "\n",
    "            print(f\"Sampled: none-ratio: {none_ratio}, removed: {total_samples - max_none_to_keep}\")\n",
    "\n",
    "        # Create and train Random Forest model\n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(train_X_fold, train_y_fold)\n",
    "        \n",
    "        classifiers.append(model)\n",
    "        \n",
    "        # Calculate score and save predictions on validation set\n",
    "        val_preds = model.predict(val_X_fold)\n",
    "        score = np.mean(val_preds == val_y_fold)\n",
    "        scores.append(score)\n",
    "        val_predictions.append({\n",
    "            'true_labels': val_y_fold,\n",
    "            'predicted_labels': val_preds,\n",
    "            'fold_indices': fold['val']\n",
    "        })\n",
    "        print(f\"Fold score: {score:.4f}\")\n",
    "\n",
    "    print(f\"Average score: {np.mean(scores)}\")\n",
    "    return classifiers, scores, val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm_classifier(train_X, _label, folds, drop_none=False):\n",
    "    \"\"\"\n",
    "    Train LightGBM models using k-fold cross validation\n",
    "\n",
    "    Args:\n",
    "        train_X: Training features DataFrame\n",
    "        _label: Array of labels\n",
    "        folds: List of dictionaries containing train/val indices\n",
    "        drop_none: Whether to drop samples with \"None\" labels\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of trained classifiers, list of scores, list of validation predictions)\n",
    "    \"\"\"\n",
    "    classifiers = []\n",
    "    scores = []\n",
    "    val_predictions = []  # List to store validation predictions\n",
    "\n",
    "    # Define LightGBM parameters\n",
    "    params = {\n",
    "        'verbose':-1,\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': 8,  # Use all available cores\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': len(set(_label))  # Set number of classes based on unique labels\n",
    "    }\n",
    "\n",
    "    for f_idx, fold in enumerate(folds):\n",
    "        # Prepare train and validation data for this fold\n",
    "        train_X_fold = train_X.iloc[fold['train']]\n",
    "        train_y_fold = _label[fold['train']]\n",
    "        val_X_fold = train_X.iloc[fold['val']]\n",
    "        val_y_fold = _label[fold['val']]\n",
    "\n",
    "        if drop_none:\n",
    "            # Remove samples with \"None\" labels from training set\n",
    "            train_mask = train_y_fold != \"None\"\n",
    "            train_X_fold = train_X_fold[train_mask]\n",
    "            train_y_fold = train_y_fold[train_mask]\n",
    "\n",
    "            # Remove samples with \"None\" labels from validation set\n",
    "            val_mask = val_y_fold != \"None\"\n",
    "            val_X_fold = val_X_fold[val_mask]\n",
    "            val_y_fold = val_y_fold[val_mask]\n",
    "            print(f\"Dropped train: {len(train_X_fold) - sum(train_mask)}, val: {len(val_X_fold) - sum(val_mask)}\")\n",
    "\n",
    "        # Check the train_y_fold. If more than 30% of samples are labeled \"None\",\n",
    "        # randomly sample from the \"None\" to make that ratio no more than 30%.\n",
    "        none_mask = (train_y_fold == \"None\")\n",
    "        none_count = np.sum(none_mask)\n",
    "        total_samples = len(train_y_fold)\n",
    "        none_ratio = none_count / total_samples if total_samples > 0 else 0\n",
    "\n",
    "        if none_ratio > 0.4:\n",
    "            # Calculate how many \"None\" labels we should keep (30% of total)\n",
    "            max_none_to_keep = int(0.4 * (total_samples - none_count))\n",
    "\n",
    "            # Randomly choose which \"None\" labels to keep\n",
    "            none_indices = np.where(none_mask)[0]\n",
    "\n",
    "            # Fix the random seed before shuffling for reproducibility\n",
    "            rng = np.random.RandomState(f_idx)\n",
    "            rng.shuffle(none_indices)\n",
    "\n",
    "            keep_none_indices = none_indices[:max_none_to_keep]\n",
    "\n",
    "            # Indices of all non-\"None\" labels\n",
    "            other_indices = np.where(~none_mask)[0]\n",
    "\n",
    "            # Combine indices to keep and then sort\n",
    "            new_indices = np.concatenate([keep_none_indices, other_indices])\n",
    "            new_indices = np.sort(new_indices)  # Sort so we can index the DataFrame consistently\n",
    "\n",
    "            # Subset the training data\n",
    "            train_X_fold = train_X_fold.iloc[new_indices]\n",
    "            train_y_fold = train_y_fold[new_indices]\n",
    "\n",
    "            print(f\"Sampled: none-ratio: {none_ratio}, removed: {total_samples - max_none_to_keep}\")\n",
    "\n",
    "        # Create and train LightGBM model\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(train_X_fold, train_y_fold)\n",
    "\n",
    "        classifiers.append(model)\n",
    "\n",
    "        # Calculate score and save predictions on validation set\n",
    "        val_preds = model.predict(val_X_fold)\n",
    "        score = np.mean(val_preds == val_y_fold)\n",
    "        scores.append(score)\n",
    "        val_predictions.append({\n",
    "            'true_labels': val_y_fold,\n",
    "            'predicted_labels': val_preds,\n",
    "            'fold_indices': fold['val']\n",
    "        })\n",
    "        print(f\"Fold score: {score:.4f}\")\n",
    "\n",
    "    print(f\"Average score: {np.mean(scores)}\")\n",
    "    return classifiers, scores, val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(train_X, _label, folds, drop_none=False):\n",
    "    \"\"\"\n",
    "    Train XGBoost models using k-fold cross-validation\n",
    "\n",
    "    Args:\n",
    "        train_X: Training features DataFrame\n",
    "        _label: Array of labels\n",
    "        folds: List of dictionaries containing train/val indices\n",
    "        drop_none: Whether to drop samples with \"None\" labels\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of trained classifiers, list of scores, list of validation predictions)\n",
    "    \"\"\"\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    classifiers = []\n",
    "    scores = []\n",
    "    val_predictions = []  # List to store validation predictions\n",
    "\n",
    "    # Define XGBoost parameters\n",
    "    params = {\n",
    "        'n_estimators': 400,       # Number of trees\n",
    "        'learning_rate': 0.3,     # Default learning rate\n",
    "        'max_depth': 6,           # Maximum depth of trees\n",
    "        'min_child_weight': 1,    # Minimum sum of weights in a child node\n",
    "        'subsample': 0.8,         # Fraction of samples per tree\n",
    "        'colsample_bytree': 0.8,  # Fraction of features per tree\n",
    "        'gamma': 0,               # Minimum loss reduction for split\n",
    "        'reg_alpha': 0,           # L1 regularization term\n",
    "        'reg_lambda': 1,          # L2 regularization term\n",
    "        'random_state': 42,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss',\n",
    "        'n_jobs': 8\n",
    "    }\n",
    "\n",
    "    for f_idx, fold in enumerate(folds):\n",
    "        # Prepare train and validation data for this fold\n",
    "        train_X_fold = train_X.iloc[fold['train']]\n",
    "        train_y_fold = _label[fold['train']]\n",
    "        val_X_fold = train_X.iloc[fold['val']]\n",
    "        val_y_fold = _label[fold['val']]\n",
    "        \n",
    "        if drop_none:\n",
    "            # Remove samples with \"None\" labels from training set\n",
    "            train_mask = train_y_fold != \"None\"\n",
    "            train_X_fold = train_X_fold[train_mask]\n",
    "            train_y_fold = train_y_fold[train_mask]\n",
    "            \n",
    "            # Remove samples with \"None\" labels from validation set\n",
    "            val_mask = val_y_fold != \"None\"\n",
    "            val_X_fold = val_X_fold[val_mask]\n",
    "            val_y_fold = val_y_fold[val_mask]\n",
    "            print(f\"Dropped train: {len(train_X_fold) - sum(train_mask)}, val: {len(val_X_fold) - sum(val_mask)}\")\n",
    "\n",
    "        # Check the train_y_fold. If more than 40% of samples are labeled \"None\",\n",
    "        # randomly sample from the \"None\" to make that ratio no more than 40%.\n",
    "        none_mask = (train_y_fold == \"None\")\n",
    "        none_count = np.sum(none_mask)\n",
    "        total_samples = len(train_y_fold)\n",
    "        none_ratio = none_count / total_samples if total_samples > 0 else 0\n",
    "\n",
    "        if none_ratio > 0.4:\n",
    "            # Calculate how many \"None\" labels we should keep (40% of total)\n",
    "            max_none_to_keep = int(0.4 * (total_samples - none_count))\n",
    "\n",
    "            # Randomly choose which \"None\" labels to keep\n",
    "            none_indices = np.where(none_mask)[0]\n",
    "\n",
    "            # Fix the random seed before shuffling for reproducibility\n",
    "            rng = np.random.RandomState(f_idx)\n",
    "            rng.shuffle(none_indices)\n",
    "            \n",
    "            keep_none_indices = none_indices[:max_none_to_keep]\n",
    "\n",
    "            # Indices of all non-\"None\" labels\n",
    "            other_indices = np.where(~none_mask)[0]\n",
    "\n",
    "            # Combine indices to keep and then sort\n",
    "            new_indices = np.concatenate([keep_none_indices, other_indices])\n",
    "            new_indices = np.sort(new_indices)  # Sort so we can index the DataFrame consistently\n",
    "\n",
    "            # Subset the training data\n",
    "            train_X_fold = train_X_fold.iloc[new_indices]\n",
    "            train_y_fold = train_y_fold[new_indices]\n",
    "\n",
    "            print(f\"Sampled: none-ratio: {none_ratio}, removed: {total_samples - max_none_to_keep}\")\n",
    "\n",
    "        # Map train_y_fold to numeric values\n",
    "        unique_classes = np.unique(train_y_fold)\n",
    "        class_to_int = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "        int_to_class = {idx: cls for cls, idx in class_to_int.items()}\n",
    "        train_y_fold = np.array([class_to_int[label] for label in train_y_fold])\n",
    "        val_y_fold_numeric = np.array([class_to_int[label] for label in val_y_fold])\n",
    "\n",
    "        # Create and train XGBoost model\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(train_X_fold, train_y_fold)\n",
    "        \n",
    "        classifiers.append(model)\n",
    "        \n",
    "        # Calculate score and save predictions on validation set\n",
    "        val_preds = model.predict(val_X_fold)\n",
    "        val_preds_labels = np.array([int_to_class[pred] for pred in val_preds])\n",
    "        score = np.mean(val_preds_labels == val_y_fold)\n",
    "        scores.append(score)\n",
    "        val_predictions.append({\n",
    "            'true_labels': val_y_fold,\n",
    "            'predicted_labels': val_preds_labels,\n",
    "            'fold_indices': fold['val']\n",
    "        })\n",
    "        print(f\"Fold score: {score:.4f}\")\n",
    "\n",
    "    print(f\"Average score: {np.mean(scores)}\")\n",
    "    return classifiers, scores, val_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the high precision model by allowing None prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clearing memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /Users/joezhao/miniconda3/envs/ddm/lib/python3.9/site-packages (5.9.8)\n"
     ]
    }
   ],
   "source": [
    "# !pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pympler\n",
      "  Downloading Pympler-1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading Pympler-1.1-py3-none-any.whl (165 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.8/165.8 kB\u001b[0m \u001b[31m227.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pympler\n",
      "Successfully installed pympler-1.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install pympler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        types |   # objects |   total size\n",
      "============================= | =========== | ============\n",
      "                numpy.ndarray |         180 |    688.70 MB\n",
      "  pandas.core.frame.DataFrame |           3 |    641.53 MB\n",
      "                         list |     1296805 |    140.27 MB\n",
      "                          str |      554889 |     71.07 MB\n",
      "                         dict |      104051 |     32.39 MB\n",
      "    pandas.core.series.Series |           1 |     26.80 MB\n",
      "                         code |       72842 |     12.39 MB\n",
      "                         type |        9106 |      7.99 MB\n",
      "                        tuple |       68311 |      3.84 MB\n",
      "                          set |        6120 |      2.79 MB\n",
      "                      weakref |       14059 |    988.52 KB\n",
      "   builtin_function_or_method |       14052 |    988.03 KB\n",
      "                          int |       32200 |    940.24 KB\n",
      "                         cell |       21427 |    836.99 KB\n",
      "                  abc.ABCMeta |         755 |    796.60 KB\n"
     ]
    }
   ],
   "source": [
    "from pympler import muppy, summary\n",
    "\n",
    "# Get a summary of all objects in memory\n",
    "all_objects = muppy.get_objects()\n",
    "memory_summary = summary.summarize(all_objects)\n",
    "\n",
    "# Print the summary\n",
    "summary.print_(memory_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 17.18 GB\n",
      "Available Memory: 7.25 GB\n",
      "Used Memory: 8.65 GB\n",
      "Memory Usage: 57.8%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Get total, available, and used memory\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total Memory: {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available Memory: {mem.available / 1e9:.2f} GB\")\n",
    "print(f\"Used Memory: {mem.used / 1e9:.2f} GB\")\n",
    "print(f\"Memory Usage: {mem.percent}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318390"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318390"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array([x[i] for x in padded_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and saving models (commented out)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training level 0\n",
      "Fold score: 0.8138\n",
      "Fold score: 0.8148\n",
      "Fold score: 0.8151\n",
      "Fold score: 0.8170\n",
      "Fold score: 0.8168\n",
      "Fold score: 0.8154\n",
      "Fold score: 0.8133\n",
      "Fold score: 0.8176\n",
      "Fold score: 0.8161\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     _classifiers, _scores, _val_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_random_forest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpadded_label\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     prec_rf_classifiers\u001b[38;5;241m.\u001b[39mappend(_classifiers)\n\u001b[1;32m      9\u001b[0m     prec_scores\u001b[38;5;241m.\u001b[39mappend(_scores)\n",
      "Cell \u001b[0;32mIn[24], line 79\u001b[0m, in \u001b[0;36mtrain_random_forest\u001b[0;34m(train_X, _label, folds, drop_none)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Create and train Random Forest model\u001b[39;00m\n\u001b[1;32m     78\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 79\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m classifiers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Calculate score and save predictions on validation set\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddm/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddm/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddm/lib/python3.9/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddm/lib/python3.9/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddm/lib/python3.9/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddm/lib/python3.9/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prec_rf_classifiers = []\n",
    "prec_scores = []\n",
    "prec_rf_val_predictions = []\n",
    "\n",
    "for i in range(1):\n",
    "    print(f\"Training level {i}\")\n",
    "    _classifiers, _scores, _val_predictions = train_random_forest(train_X, np.array([x[i] for x in padded_label]), folds, drop_none=False)\n",
    "    prec_rf_classifiers.append(_classifiers)\n",
    "    prec_scores.append(_scores)\n",
    "    prec_rf_val_predictions.append(_val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Loop through levels\n",
    "for level in range(len(prec_rf_classifiers)):  # Iterate over levels (5 in this case)\n",
    "    classifiers = prec_rf_classifiers[level]  # List of classifiers for this level\n",
    "    for fold_idx, classifier in enumerate(classifiers):  # Iterate over folds\n",
    "        # Define the file path for this classifier\n",
    "        file_path = f\"./rf_classifier_level_0_fold_{fold_idx}.pkl\"\n",
    "        \n",
    "        # Save the classifier using pickle\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(classifier, f)\n",
    "        print(f\"Saved RF classifier for level 0, fold {fold_idx} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training level 4\n",
      "Sampled: none-ratio: 0.9884523173885277, removed: 285228\n",
      "Fold score: 0.9479\n",
      "Sampled: none-ratio: 0.9884313787074552, removed: 285225\n",
      "Fold score: 0.9537\n",
      "Sampled: none-ratio: 0.9884313787074552, removed: 285225\n",
      "Fold score: 0.9475\n",
      "Sampled: none-ratio: 0.9883197057417353, removed: 285213\n",
      "Fold score: 0.9539\n",
      "Sampled: none-ratio: 0.9883615831038803, removed: 285217\n",
      "Fold score: 0.9440\n",
      "Sampled: none-ratio: 0.9883511137633441, removed: 285216\n",
      "Fold score: 0.9497\n",
      "Sampled: none-ratio: 0.9884592969488852, removed: 285229\n",
      "Fold score: 0.9546\n",
      "Sampled: none-ratio: 0.988406950246204, removed: 285223\n",
      "Fold score: 0.9421\n",
      "Sampled: none-ratio: 0.9884313787074552, removed: 285225\n",
      "Fold score: 0.9516\n",
      "Sampled: none-ratio: 0.9884592969488852, removed: 285229\n",
      "Fold score: 0.9543\n",
      "Average score: 0.9499167687427368\n"
     ]
    }
   ],
   "source": [
    "prec_lgbm_classifiers = []\n",
    "prec_scores = []\n",
    "prec_lgbm_val_predictions = []\n",
    "\n",
    "for i in range(4,5):\n",
    "    print(f\"Training level {i}\")\n",
    "    if (i == 1) or (i == 2) or (i == 3):\n",
    "        _classifiers, _scores, _val_predictions = train_random_forest(train_X, np.array([x[i] for x in padded_label]), folds, drop_none=False)\n",
    "    else:\n",
    "        _classifiers, _scores, _val_predictions = train_lgbm_classifier(train_X, np.array([x[i] for x in padded_label]), folds, drop_none=False)\n",
    "    prec_lgbm_classifiers.append(_classifiers)\n",
    "    prec_scores.append(_scores)\n",
    "    prec_lgbm_val_predictions.append(_val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Loop through levels\n",
    "# for level in range(len(prec_lgbm_classifiers)):  # Iterate over levels (5 in this case)\n",
    "#     classifiers = prec_lgbm_classifiers[level]  # List of classifiers for this level\n",
    "#     for fold_idx, classifier in enumerate(classifiers):  # Iterate over folds\n",
    "#         # Define the file path for this classifier\n",
    "#         file_path = f\"./lgbm_classifier_level_4_fold_{fold_idx}.pkl\"\n",
    "        \n",
    "#         # Save the classifier using pickle\n",
    "#         with open(file_path, \"wb\") as f:\n",
    "#             pickle.dump(classifier, f)\n",
    "#         print(f\"Saved LGBM classifier for level 4, fold {fold_idx} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prec_xgb_classifiers = []\n",
    "# prec_scores = []\n",
    "# prec_xgb_val_predictions = []\n",
    "\n",
    "# for i in range(5):\n",
    "#     print(f\"Training level {i}\")\n",
    "#     _classifiers, _scores, _val_predictions = train_xgboost(train_X, np.array([x[i] for x in padded_label]), folds, drop_none=False)\n",
    "#     prec_xgb_classifiers.append(_classifiers)\n",
    "#     prec_scores.append(_scores)\n",
    "#     prec_xgb_val_predictions.append(_val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Loop through levels\n",
    "# for level in range(len(prec_xgb_classifiers)):  # Iterate over levels (5 in this case)\n",
    "#     classifiers = prec_xgb_classifiers[level]  # List of classifiers for this level\n",
    "#     for fold_idx, classifier in enumerate(classifiers):  # Iterate over folds\n",
    "#         # Define the file path for this classifier\n",
    "#         file_path = f\"./xgb_classifier_level_{level}_fold_{fold_idx}.pkl\"\n",
    "        \n",
    "#         # Save the classifier using pickle\n",
    "#         with open(file_path, \"wb\") as f:\n",
    "#             pickle.dump(classifier, f)\n",
    "#         print(f\"Saved XGBoost classifier for level {level}, fold {fold_idx} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stack all 5 folds' predictions and true labels together\n",
    "# all_true_labels = np.concatenate([_fold['true_labels'] for _fold in prec_rf_val_predictions[2]])\n",
    "# all_predicted_labels = np.concatenate([_fold['predicted_labels'] for _fold in prec_rf_val_predictions[2]])\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "# # Create a figure with larger size\n",
    "# plt.figure(figsize=(20, 16))\n",
    "\n",
    "# # Create heatmap\n",
    "# sns.heatmap(cm, \n",
    "#             xticklabels=np.unique(all_true_labels),\n",
    "#             yticklabels=np.unique(all_true_labels),\n",
    "#             annot=True, \n",
    "#             fmt='d',\n",
    "#             cmap='Blues')\n",
    "\n",
    "# plt.title('Confusion Matrix - Fold 0')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Print classification report for more detailed metrics\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(all_true_labels, all_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('random_forest_data.pkl', 'rb') as f:\n",
    "#    data = pickle.load(f)\n",
    "#prec_rf_classifiers = data['classifiers1']\n",
    "#prec_lgbm_classifiers = data['classifiers2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stack all 5 folds' predictions and true labels together\n",
    "# all_true_labels = np.concatenate([_fold['true_labels'] for _fold in prec_lgbm_val_predictions[2]])\n",
    "# all_predicted_labels = np.concatenate([_fold['predicted_labels'] for _fold in prec_lgbm_val_predictions[2]])\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cm = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "# # Create a figure with larger size\n",
    "# plt.figure(figsize=(20, 16))\n",
    "\n",
    "# # Create heatmap\n",
    "# sns.heatmap(cm, \n",
    "#             xticklabels=np.unique(all_true_labels),\n",
    "#             yticklabels=np.unique(all_true_labels),\n",
    "#             annot=True, \n",
    "#             fmt='d',\n",
    "#             cmap='Blues')\n",
    "\n",
    "# plt.title('Confusion Matrix - Fold 0')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Print classification report for more detailed metrics\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(all_true_labels, all_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_value = np.finfo(np.float32).max\n",
    "#cliped_text_X = np.clip(test_X, -max_value, max_value)\n",
    "#cliped_text_X = np.clip(test_X, a_min=None, a_max=np.finfo(np.float32).max)\n",
    "#cliped_text_X = np.nan_to_num(test_X, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.read_csv('./data_features_fix/test_features_full_v3.csv', index_col=0)\n",
    "test_X_1 = pd.read_csv('./data_features_fix/test_features_split1_2_v3.csv', index_col=0)\n",
    "test_X_2 = pd.read_csv('./data_features_fix/test_features_split2_2_v3.csv', index_col=0)\n",
    "test_X_3 = pd.read_csv('./data_features_fix/test_features_split1_3_v3.csv', index_col=0)\n",
    "test_X_4 = pd.read_csv('./data_features_fix/test_features_split2_3_v3.csv', index_col=0)\n",
    "test_X_5 = pd.read_csv('./data_features_fix/test_features_split3_3_v3.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = np.finfo(np.float32).max\n",
    "test_X = np.clip(test_X, -max_value, max_value)\n",
    "test_X = test_X.replace([np.inf, -np.inf], 0.0)  # Replace infinity\n",
    "test_X = test_X.fillna(0.0)  # Replace NaN\n",
    "test_X_1 = np.clip(test_X_1, -max_value, max_value)\n",
    "test_X_1 = test_X_1.replace([np.inf, -np.inf], 0.0)  # Replace infinity\n",
    "test_X_1 = test_X_1.fillna(0.0)  # Replace NaN\n",
    "test_X_2 = np.clip(test_X_2, -max_value, max_value)\n",
    "test_X_2 = test_X_2.replace([np.inf, -np.inf], 0.0)  # Replace infinity\n",
    "test_X_2 = test_X_2.fillna(0.0)  # Replace NaN\n",
    "test_X_3 = np.clip(test_X_1, -max_value, max_value)\n",
    "test_X_3 = test_X_3.replace([np.inf, -np.inf], 0.0)  # Replace infinity\n",
    "test_X_3 = test_X_3.fillna(0.0)  # Replace NaN\n",
    "test_X_4 = np.clip(test_X_4, -max_value, max_value)\n",
    "test_X_4 = test_X_4.replace([np.inf, -np.inf], 0.0)  # Replace infinity\n",
    "test_X_4 = test_X_4.fillna(0.0)  # Replace NaN\n",
    "test_X_5 = np.clip(test_X_5, -max_value, max_value)\n",
    "test_X_5 = test_X_5.replace([np.inf, -np.inf], 0.0)  # Replace infinity\n",
    "test_X_5 = test_X_5.fillna(0.0)  # Replace NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cliped_text_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cliped_text_X = cliped_text_X.replace([np.inf, -np.inf], 0.0)  # Replace infinity\n",
    "cliped_text_X = cliped_text_X.fillna(0.0)  # Replace NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Checking for NaN values:\", np.any(np.isnan(cliped_text_X)))\n",
    "print(\"Checking for infinite values:\", np.any(np.isinf(cliped_text_X)))\n",
    "print(\"Maximum value:\", np.max(cliped_text_X))\n",
    "print(\"Minimum value:\", np.min(cliped_text_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cliped_text_X[cliped_text_X.isnull().any(axis=1)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the downloaded classifiers\n",
    "\n",
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classifiers successfully loaded into `prec_rf_classifiers`.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Initialize the 2D list to store classifiers\n",
    "prec_rf_classifiers = []\n",
    "\n",
    "# Loop through each level\n",
    "for level in range(5):\n",
    "    level_classifiers = []  # List to store classifiers for the current level\n",
    "    \n",
    "    # Loop through each fold for the current level\n",
    "    for fold in range(10):\n",
    "        # Construct the filename\n",
    "        file_path = f'./rf-models-default-params/rf_classifier_level_{level}_fold_{fold}.pkl'\n",
    "        \n",
    "        # Load the classifier\n",
    "        with open(file_path, 'rb') as f:\n",
    "            classifier = pickle.load(f)\n",
    "        \n",
    "        # Add the classifier to the list for the current level\n",
    "        level_classifiers.append(classifier)\n",
    "    \n",
    "    # Append the list of classifiers for the current level to the main list\n",
    "    prec_rf_classifiers.append(level_classifiers)\n",
    "\n",
    "# The `prec_rf_classifiers` list now contains all classifiers organized by level and fold\n",
    "print(\"All classifiers successfully loaded into `prec_rf_classifiers`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prec_rf_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_rf_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classifiers successfully loaded into `prec_lgbm_classifiers`.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Initialize the 2D list to store classifiers\n",
    "prec_lgbm_classifiers = []\n",
    "\n",
    "# Loop through each level\n",
    "for level in range(5):\n",
    "    level_classifiers = []  # List to store classifiers for the current level\n",
    "    \n",
    "    # Loop through each fold for the current level\n",
    "    for fold in range(10):\n",
    "        # Construct the filename\n",
    "        if (level == 1) or (level == 2) or (level == 3):\n",
    "            level_classifiers.append(prec_rf_classifiers[level][fold])\n",
    "            continue\n",
    "        else:\n",
    "            file_path = f'./lgbm-models-default-params/lgbm_classifier_level_{level}_fold_{fold}.pkl'\n",
    "        \n",
    "        # Load the classifier\n",
    "        with open(file_path, 'rb') as f:\n",
    "            classifier = pickle.load(f)\n",
    "        \n",
    "        # Add the classifier to the list for the current level\n",
    "        level_classifiers.append(classifier)\n",
    "    \n",
    "    # Append the list of classifiers for the current level to the main list\n",
    "    prec_lgbm_classifiers.append(level_classifiers)\n",
    "\n",
    "# The `prec_rf_classifiers` list now contains all classifiers organized by level and fold\n",
    "print(\"All classifiers successfully loaded into `prec_lgbm_classifiers`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=6, objective='multiclass', random_state=42,\n",
       "                 verbose=-1)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42),\n",
       "  RandomForestClassifier(n_jobs=8, random_state=42)],\n",
       " [LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1),\n",
       "  LGBMClassifier(n_jobs=8, num_class=9, objective='multiclass', random_state=42,\n",
       "                 verbose=-1)]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_lgbm_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prec_lgbm_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classifiers successfully loaded into `xgb_rf_classifiers`.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Initialize the 2D list to store classifiers\n",
    "prec_xgb_classifiers = []\n",
    "\n",
    "# Loop through each level\n",
    "for level in range(5):\n",
    "    level_classifiers = []  # List to store classifiers for the current level\n",
    "    \n",
    "    # Loop through each fold for the current level\n",
    "    for fold in range(10):\n",
    "        # Construct the filename\n",
    "        file_path = f'./xgb-models-400-estimators-0.3-lr/xgb_classifier_level_{level}_fold_{fold}.pkl'\n",
    "        \n",
    "        # Load the classifier\n",
    "        with open(file_path, 'rb') as f:\n",
    "            classifier = pickle.load(f)\n",
    "        \n",
    "        # Add the classifier to the list for the current level\n",
    "        level_classifiers.append(classifier)\n",
    "    \n",
    "    # Append the list of classifiers for the current level to the main list\n",
    "    prec_xgb_classifiers.append(level_classifiers)\n",
    "\n",
    "# The `prec_rf_classifiers` list now contains all classifiers organized by level and fold\n",
    "print(\"All classifiers successfully loaded into `xgb_rf_classifiers`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...)],\n",
       " [XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...)],\n",
       " [XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...)],\n",
       " [XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...)],\n",
       " [XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "                max_leaves=None, min_child_weight=1, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=400,\n",
       "                n_jobs=8, num_parallel_tree=None, objective='multi:softprob', ...)]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_xgb_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prec_xgb_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_with_models_vote(classifiers1, classifiers2, classifiers3, test_data, test_data2, \n",
    "                                      test_data3, test_data4, test_data5, test_data6 ):\n",
    "    \"\"\"\n",
    "    Make probability predictions using multiple classifier models\n",
    "    \n",
    "    Args:\n",
    "        classifiers: List of trained classifier models\n",
    "        test_data: Test data to make predictions on\n",
    "        \n",
    "    Returns:\n",
    "        List of probability predictions from each classifier\n",
    "    \"\"\"\n",
    "    test_preds_all = []\n",
    "    for i in tqdm(range(len(classifiers1))):\n",
    "    #for clf in tqdm(classifiers):\n",
    "        pred1 = classifiers1[i].predict_proba(test_data)\n",
    "        pred2 = classifiers2[i].predict_proba(test_data)\n",
    "        pred3 = classifiers3[i].predict_proba(test_data)\n",
    "        pred4 = classifiers1[i].predict_proba(test_data2)\n",
    "        pred5 = classifiers2[i].predict_proba(test_data2)\n",
    "        pred6 = classifiers3[i].predict_proba(test_data2)\n",
    "        pred7 = classifiers1[i].predict_proba(test_data3)\n",
    "        pred8 = classifiers2[i].predict_proba(test_data3)\n",
    "        pred9 = classifiers3[i].predict_proba(test_data3)\n",
    "        pred10 = classifiers1[i].predict_proba(test_data4)\n",
    "        pred11 = classifiers2[i].predict_proba(test_data4)\n",
    "        pred12 = classifiers3[i].predict_proba(test_data4)\n",
    "        pred13 = classifiers1[i].predict_proba(test_data5)\n",
    "        pred14 = classifiers2[i].predict_proba(test_data5)\n",
    "        pred15 = classifiers3[i].predict_proba(test_data5)\n",
    "        pred16 = classifiers1[i].predict_proba(test_data6)\n",
    "        pred17 = classifiers2[i].predict_proba(test_data6)\n",
    "        pred18 = classifiers3[i].predict_proba(test_data6)\n",
    "        pred = (pred1+pred2+pred3+pred4+pred5+pred6+pred7+pred8+\n",
    "                pred9+pred10+pred11+pred12+pred13+pred14+pred15+\n",
    "               pred16+pred17+pred18)/18.0\n",
    "        test_preds_all.append(pred)\n",
    "    return test_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_combine_predictions(classifiers1, classifiers2, classifiers3, test_preds_all, test_data, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Aligns predictions from multiple classifiers and combines them through averaging\n",
    "    \n",
    "    Args:\n",
    "        classifiers: List of trained classifier models\n",
    "        test_preds_all: List of probability predictions from each classifier\n",
    "        test_data: Test data used for predictions\n",
    "        threshold: Minimum probability threshold for making predictions\n",
    "        \n",
    "    Returns:\n",
    "        Final class predictions after aligning and combining probabilities\n",
    "    \"\"\"\n",
    "    # Get the common classes across all classifiers\n",
    "    all_classes = classifiers2[0].classes_\n",
    "    test_preds_aligned = []\n",
    "\n",
    "    # Make predictions with each fold's model and align them \n",
    "    for i, clf in tqdm(enumerate(classifiers2)):\n",
    "        pred = test_preds_all[i]\n",
    "        # Create a mapping to align predictions with common classes\n",
    "        pred_dict = {_cls: idx for idx, _cls in enumerate(clf.classes_)}\n",
    "        aligned_pred = np.zeros((len(test_data), len(all_classes)))\n",
    "        \n",
    "        for i, _cls in enumerate(all_classes):\n",
    "            if _cls in pred_dict:\n",
    "                aligned_pred[:, i] = pred[:, pred_dict[_cls]]\n",
    "        \n",
    "        test_preds_aligned.append(aligned_pred)\n",
    "\n",
    "    # Stack and average the aligned predictions\n",
    "    test_preds_all = np.stack(test_preds_aligned)\n",
    "    test_preds_proba = test_preds_all.mean(axis=0)\n",
    "\n",
    "    # Get max probabilities for each prediction\n",
    "    max_probs = np.max(test_preds_proba, axis=1)\n",
    "    \n",
    "    # Convert probabilities to class predictions, using threshold\n",
    "    test_preds = np.array(['None'] * len(test_data), dtype=object)\n",
    "    confident_mask = max_probs >= threshold\n",
    "    test_preds[confident_mask] = all_classes[np.argmax(test_preds_proba[confident_mask], axis=1)]\n",
    "    \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proba(classifiers1, classifiers2, classifiers3, test_preds_all, test_data, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Aligns predictions from multiple classifiers and combines them through averaging\n",
    "    \n",
    "    Args:\n",
    "        classifiers: List of trained classifier models\n",
    "        test_preds_all: List of probability predictions from each classifier\n",
    "        test_data: Test data used for predictions\n",
    "        threshold: Minimum probability threshold for making predictions\n",
    "        \n",
    "    Returns:\n",
    "        Final class predictions after aligning and combining probabilities\n",
    "    \"\"\"\n",
    "    # Get the common classes across all classifiers\n",
    "    all_classes = classifiers1[0].classes_\n",
    "    test_preds_aligned = []\n",
    "\n",
    "    # Make predictions with each fold's model and align them \n",
    "    for i, clf in tqdm(enumerate(classifiers1)):\n",
    "        pred = test_preds_all[i]\n",
    "        # Create a mapping to align predictions with common classes\n",
    "        pred_dict = {_cls: idx for idx, _cls in enumerate(clf.classes_)}\n",
    "        aligned_pred = np.zeros((len(test_data), len(all_classes)))\n",
    "        \n",
    "        for i, _cls in enumerate(all_classes):\n",
    "            if _cls in pred_dict:\n",
    "                aligned_pred[:, i] = pred[:, pred_dict[_cls]]\n",
    "        \n",
    "        test_preds_aligned.append(aligned_pred)\n",
    "\n",
    "    # Stack and average the aligned predictions\n",
    "    test_preds_all = np.stack(test_preds_aligned)\n",
    "    test_preds_proba = test_preds_all.mean(axis=0)\n",
    "\n",
    "    ## Get max probabilities for each prediction\n",
    "    #max_probs = np.max(test_preds_proba, axis=1)\n",
    "    \n",
    "    # Convert probabilities to class predictions, using threshold\n",
    "    #test_preds = np.array(['None'] * len(test_data), dtype=object)\n",
    "    #confident_mask = max_probs >= threshold\n",
    "    #test_preds[confident_mask] = all_classes[np.argmax(test_preds_proba[confident_mask], axis=1)]\n",
    "    \n",
    "    return test_preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_preds = []\n",
    "thr_list = [0.0, 0.2, 0.3, 0.4, 0.5]\n",
    "for i in range(5):\n",
    "    print(f\"Predicting level {i}\")\n",
    "    test_preds_all = make_predictions_with_models(prec_classifiers[i], cliped_text_X)\n",
    "    test_preds.append(align_and_combine_predictions(prec_classifiers[i], test_preds_all, cliped_text_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_preds = []\n",
    "thr_list = [0.0, 0.2, 0.3, 0.4, 0.5]\n",
    "for i in range(5):\n",
    "    print(f\"Predicting level {i}\")\n",
    "    test_preds_all = make_predictions_with_models_vote(prec_rf_classifiers[i], prec_lgbm_classifiers[i], prec_xgb_classifiers[i], cliped_text_X)\n",
    "    test_preds.append(align_and_combine_predictions(prec_rf_classifiers[i], prec_lgbm_classifiers[i], prec_xgb_classifiers[i], test_preds_all, cliped_text_X))\n",
    "    #test_preds_all = make_predictions_with_models_vote(prec_rf_classifiers[i], prec_lgbm_classifiers[i], prec_xgb_classifiers[i], test_X)\n",
    "    #test_preds.append(align_and_combine_predictions(prec_rf_classifiers[i], prec_lgbm_classifiers[i], prec_xgb_classifiers[i], test_preds_all, test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_preds_all[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to array and process None values\n",
    "stacked = np.stack(test_preds).transpose()\n",
    "for row in tqdm(stacked):\n",
    "    # Find first occurrence of 'None' if any\n",
    "    none_idx = np.where(row == 'None')[0]\n",
    "    if len(none_idx) > 0:\n",
    "        # Set all elements after first None to None\n",
    "        first_none = none_idx[0]\n",
    "        row[first_none:] = 'None'\n",
    "        \n",
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting level 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, 104.09it/s]███████████████████████████████████████████████████████████████████████████| 10/10 [05:27<00:00, 32.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting level 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:01,  5.06it/s]███████████████████████████████████████████████████████████████████████████| 10/10 [16:48<00:00, 100.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting level 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:01,  8.11it/s]████████████████████████████████████████████████████████████████████████████| 10/10 [12:35<00:00, 75.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting level 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, 12.00it/s]████████████████████████████████████████████████████████████████████████████| 10/10 [08:18<00:00, 49.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting level 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, 50.73it/s]████████████████████████████████████████████████████████████████████████████| 10/10 [04:59<00:00, 30.00s/it]"
     ]
    }
   ],
   "source": [
    "#gets proba\n",
    "test_preds_all_list = []\n",
    "thr_list = [0.0, 0.2, 0.3, 0.4, 0.5]\n",
    "for i in range(5):\n",
    "    print(f\"Predicting level {i}\")\n",
    "    test_preds_all = make_predictions_with_models_vote(prec_rf_classifiers[i], prec_lgbm_classifiers[i], prec_xgb_classifiers[i], \n",
    "                                                       test_X, test_X_1, test_X_2, test_X_3, test_X_4, test_X_5)\n",
    "    test_preds_all_list.append(get_proba(prec_rf_classifiers[i], prec_lgbm_classifiers[i], prec_xgb_classifiers[i], test_preds_all, test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tier0</th>\n",
       "      <th>tier1</th>\n",
       "      <th>tier2</th>\n",
       "      <th>tier3</th>\n",
       "      <th>tier4</th>\n",
       "      <th>combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ParameterNoneNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Setpoint</td>\n",
       "      <td>Temperature_Setpoint</td>\n",
       "      <td>Cooling_Temperature_Setpoint</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SetpointTemperature_SetpointCooling_Temperatur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Current_Sensor</td>\n",
       "      <td>Load_Current_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorCurrent_SensorLoad_Current_SensorNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Power_Sensor</td>\n",
       "      <td>Electrical_Power_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorPower_SensorElectrical_Power_SensorNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Setpoint</td>\n",
       "      <td>Speed_Setpoint</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SetpointSpeed_SetpointNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6699</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Power_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorPower_SensorNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>7929</td>\n",
       "      <td>Setpoint</td>\n",
       "      <td>Cooling_Demand_Setpoint</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SetpointCooling_Demand_SetpointNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>10899</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Temperature_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorTemperature_SensorNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>13579</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Flow_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorFlow_SensorNoneNoneNone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>14098</td>\n",
       "      <td>Sensor</td>\n",
       "      <td>Outside_Air_Enthalpy_Sensor</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SensorOutside_Air_Enthalpy_SensorNoneNoneNone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index      tier0                        tier1  \\\n",
       "0       0  Parameter                         None   \n",
       "1       1   Setpoint         Temperature_Setpoint   \n",
       "2       2     Sensor               Current_Sensor   \n",
       "3       3     Sensor                 Power_Sensor   \n",
       "4       4   Setpoint               Speed_Setpoint   \n",
       "..    ...        ...                          ...   \n",
       "86   6699     Sensor                 Power_Sensor   \n",
       "87   7929   Setpoint      Cooling_Demand_Setpoint   \n",
       "88  10899     Sensor           Temperature_Sensor   \n",
       "89  13579     Sensor                  Flow_Sensor   \n",
       "90  14098     Sensor  Outside_Air_Enthalpy_Sensor   \n",
       "\n",
       "                           tier2 tier3 tier4  \\\n",
       "0                           None  None  None   \n",
       "1   Cooling_Temperature_Setpoint  None  None   \n",
       "2            Load_Current_Sensor  None  None   \n",
       "3        Electrical_Power_Sensor  None  None   \n",
       "4                           None  None  None   \n",
       "..                           ...   ...   ...   \n",
       "86                          None  None  None   \n",
       "87                          None  None  None   \n",
       "88                          None  None  None   \n",
       "89                          None  None  None   \n",
       "90                          None  None  None   \n",
       "\n",
       "                                              combine  \n",
       "0                           ParameterNoneNoneNoneNone  \n",
       "1   SetpointTemperature_SetpointCooling_Temperatur...  \n",
       "2     SensorCurrent_SensorLoad_Current_SensorNoneNone  \n",
       "3   SensorPower_SensorElectrical_Power_SensorNoneNone  \n",
       "4                  SetpointSpeed_SetpointNoneNoneNone  \n",
       "..                                                ...  \n",
       "86                     SensorPower_SensorNoneNoneNone  \n",
       "87        SetpointCooling_Demand_SetpointNoneNoneNone  \n",
       "88               SensorTemperature_SensorNoneNoneNone  \n",
       "89                      SensorFlow_SensorNoneNoneNone  \n",
       "90      SensorOutside_Air_Enthalpy_SensorNoneNoneNone  \n",
       "\n",
       "[91 rows x 7 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdlistlabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|█████████████████████████████████████████████████████████████████████████████████████| 315720/315720 [04:25<00:00, 1190.67it/s]"
     ]
    }
   ],
   "source": [
    "finalstack = []  # Initialize finalstack as a list to accumulate results\n",
    "\n",
    "for i in tqdm(range(len(test_preds_all_list[0]))):\n",
    "    rankbest = np.zeros([len(pdlistlabl), 5])  # Initialize rankbest\n",
    "    nplistlab = pdlistlabl[pdlistlabl.columns[1:-1]].to_numpy()  # Convert DataFrame to NumPy array\n",
    "    \n",
    "    for j in range(5):\n",
    "        for ii in range(len(rankbest)):  # Iterate over rows in rankbest\n",
    "            try:\n",
    "                tier_prob = test_preds_all_list[j][i]  # Get predictions for current fold\n",
    "                tier_class = prec_rf_classifiers[j][0].classes_.tolist()  # Get class labels\n",
    "                tier_idx = tier_class.index(nplistlab[ii][j])  # Get index of the label in tier_class\n",
    "                rankbest[ii, j] = tier_prob[tier_idx]  # Assign the probability to rankbest\n",
    "            except:\n",
    "                tier_prob = test_preds_all_list[j+1][i]  # Get predictions for current fold\n",
    "                tier_class = prec_rf_classifiers[j+1][0].classes_.tolist()  # Get class labels\n",
    "                tier_idx = tier_class.index(nplistlab[ii][j])  # Get index of the label in tier_class\n",
    "                rankbest[ii, j] = tier_prob[tier_idx]  # Assign the probability to rankbest\n",
    "    # Get the index of the row in nplistlab corresponding to the maximum mean rankbest value\n",
    "    selected_row = nplistlab[np.argmax(np.mean(rankbest, axis=1))]\n",
    "    finalstack.append(selected_row)  # Append the selected row to finalstack\n",
    "\n",
    "# Convert finalstack to a NumPy array if needed\n",
    "finalstack = np.array(finalstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sensor', 'Outside_Air_Enthalpy_Sensor', 'None', 'None', 'None'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nplistlab[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prec_rf_classifiers[j+1][0].classes_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Average_Zone_Air_Temperature_Sensor',\n",
       " 'Cooling_Temperature_Setpoint',\n",
       " 'Differential_Supply_Return_Water_Temperature_Sensor',\n",
       " 'Heating_Temperature_Setpoint',\n",
       " 'None',\n",
       " 'Outside_Air_Temperature_Setpoint',\n",
       " 'Peak_Power_Demand_Sensor',\n",
       " 'Return_Water_Temperature_Sensor',\n",
       " 'Warmest_Zone_Air_Temperature_Sensor']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tier_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming `number_to_label` is already defined from the previous code\n",
    "# # Create a function to map numbers to labels\n",
    "# def map_numbers_to_labels(array, mapping):\n",
    "#     for row in tqdm(array):\n",
    "#         for i, value in enumerate(row):\n",
    "#             # If the value is numeric, map it to the label\n",
    "#             if isinstance(value, (int, np.integer)):\n",
    "#                 row[i] = mapping.get(value, value)  # Keep value if not in mapping\n",
    "\n",
    "# # Example mapping dictionary (you should replace this with your actual mapping)\n",
    "# number_to_label = {i: label for i, label in enumerate(train_y.columns[1:].tolist())}\n",
    "\n",
    "# # Apply the mapping to the array\n",
    "# map_numbers_to_labels(stacked, number_to_label)\n",
    "\n",
    "# # Output the processed array\n",
    "# stacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Sensor', 'Power_Sensor', 'Electrical_Power_Sensor', 'None',\n",
       "        'None'],\n",
       "       ['Sensor', 'Flow_Sensor', 'Chilled_Water_Supply_Flow_Sensor',\n",
       "        'Water_Flow_Sensor', 'None'],\n",
       "       ['Sensor', 'Demand_Sensor', 'None', 'None', 'None'],\n",
       "       ...,\n",
       "       ['Sensor', 'None', 'None', 'None', 'None'],\n",
       "       ['Sensor', 'Power_Sensor', 'None', 'None', 'None'],\n",
       "       ['Alarm', 'None', 'None', 'None', 'None']], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnlist = ['Active_Power_Sensor', 'Air_Flow_Sensor',\n",
    "       'Air_Flow_Setpoint', 'Air_Temperature_Sensor',\n",
    "       'Air_Temperature_Setpoint', 'Alarm', 'Angle_Sensor',\n",
    "       'Average_Zone_Air_Temperature_Sensor',\n",
    "       'Chilled_Water_Differential_Temperature_Sensor',\n",
    "       'Chilled_Water_Return_Temperature_Sensor',\n",
    "       'Chilled_Water_Supply_Flow_Sensor',\n",
    "       'Chilled_Water_Supply_Temperature_Sensor', 'Command',\n",
    "       'Cooling_Demand_Sensor', 'Cooling_Demand_Setpoint',\n",
    "       'Cooling_Supply_Air_Temperature_Deadband_Setpoint',\n",
    "       'Cooling_Temperature_Setpoint', 'Current_Sensor',\n",
    "       'Damper_Position_Sensor', 'Damper_Position_Setpoint', 'Demand_Sensor',\n",
    "       'Dew_Point_Setpoint', 'Differential_Pressure_Sensor',\n",
    "       'Differential_Pressure_Setpoint',\n",
    "       'Differential_Supply_Return_Water_Temperature_Sensor',\n",
    "       'Discharge_Air_Dewpoint_Sensor', 'Discharge_Air_Temperature_Sensor',\n",
    "       'Discharge_Air_Temperature_Setpoint',\n",
    "       'Discharge_Water_Temperature_Sensor', 'Duration_Sensor',\n",
    "       'Electrical_Power_Sensor', 'Energy_Usage_Sensor',\n",
    "       'Filter_Differential_Pressure_Sensor', 'Flow_Sensor', 'Flow_Setpoint',\n",
    "       'Frequency_Sensor', 'Heating_Demand_Sensor', 'Heating_Demand_Setpoint',\n",
    "       'Heating_Supply_Air_Temperature_Deadband_Setpoint',\n",
    "       'Heating_Temperature_Setpoint', 'Hot_Water_Flow_Sensor',\n",
    "       'Hot_Water_Return_Temperature_Sensor',\n",
    "       'Hot_Water_Supply_Temperature_Sensor', 'Humidity_Setpoint',\n",
    "       'Load_Current_Sensor', 'Low_Outside_Air_Temperature_Enable_Setpoint',\n",
    "       'Max_Air_Temperature_Setpoint', 'Min_Air_Temperature_Setpoint',\n",
    "       'Outside_Air_CO2_Sensor', 'Outside_Air_Enthalpy_Sensor',\n",
    "       'Outside_Air_Humidity_Sensor',\n",
    "       'Outside_Air_Lockout_Temperature_Setpoint',\n",
    "       'Outside_Air_Temperature_Sensor', 'Outside_Air_Temperature_Setpoint',\n",
    "       'Parameter', 'Peak_Power_Demand_Sensor', 'Position_Sensor',\n",
    "       'Power_Sensor', 'Pressure_Sensor', 'Rain_Sensor',\n",
    "       'Reactive_Power_Sensor', 'Reset_Setpoint',\n",
    "       'Return_Air_Temperature_Sensor', 'Return_Water_Temperature_Sensor',\n",
    "       'Room_Air_Temperature_Setpoint', 'Sensor', 'Setpoint',\n",
    "       'Solar_Radiance_Sensor', 'Speed_Setpoint', 'Static_Pressure_Sensor',\n",
    "       'Static_Pressure_Setpoint', 'Status', 'Supply_Air_Humidity_Sensor',\n",
    "       'Supply_Air_Static_Pressure_Sensor',\n",
    "       'Supply_Air_Static_Pressure_Setpoint', 'Supply_Air_Temperature_Sensor',\n",
    "       'Supply_Air_Temperature_Setpoint', 'Temperature_Sensor',\n",
    "       'Temperature_Setpoint', 'Thermal_Power_Sensor', 'Time_Setpoint',\n",
    "       'Usage_Sensor', 'Valve_Position_Sensor', 'Voltage_Sensor',\n",
    "       'Warmest_Zone_Air_Temperature_Sensor', 'Water_Flow_Sensor',\n",
    "       'Water_Temperature_Sensor', 'Water_Temperature_Setpoint',\n",
    "       'Wind_Direction_Sensor', 'Wind_Speed_Sensor',\n",
    "       'Zone_Air_Dewpoint_Sensor', 'Zone_Air_Humidity_Sensor',\n",
    "       'Zone_Air_Humidity_Setpoint', 'Zone_Air_Temperature_Sensor'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipftest = ZipFile('./test_X_v0.1.0.zip', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "listtestfile = zipftest.namelist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|████████████████████████████████████████████████████████████████████████████████████| 315720/315720 [00:22<00:00, 13755.69it/s]"
     ]
    }
   ],
   "source": [
    "stackedfinalresult = pd.DataFrame(columns=['filename'])\n",
    "stackedfinalresult['filename'] = pd.Series(listtestfile).apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "for labelname in columnlist:\n",
    "    stackedfinalresult[labelname] = 0\n",
    "\n",
    "test_preds = finalstack\n",
    "for i in tqdm(range(len(test_preds))):\n",
    "    # stackedfinalresult.loc[i, test_preds[i]] = 1\n",
    "    predlist = test_preds[i].tolist()\n",
    "    predlist = [x for x in predlist if x != 'None']\n",
    "    for predlabelname in predlist:\n",
    "    \tstackedfinalresult.loc[i, predlabelname] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedfinalresult.loc[((stackedfinalresult['Peak_Power_Demand_Sensor'] == 1)), \n",
    "            'Demand_Sensor'] = 1\n",
    "stackedfinalresult.loc[((stackedfinalresult['Cooling_Supply_Air_Temperature_Deadband_Setpoint'] == 1)), \n",
    "            'Air_Temperature_Setpoint'] = 1\n",
    "stackedfinalresult.loc[((stackedfinalresult['Heating_Supply_Air_Temperature_Deadband_Setpoint'] == 1)), \n",
    "            'Air_Temperature_Setpoint'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedfinalresult = stackedfinalresult.assign(**{col: stackedfinalresult[col].astype(float) for col in stackedfinalresult.columns if col != \"filename\"})\n",
    "stackedfinalresult.to_csv(\"./submit/18-01_v16jonas_post.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
